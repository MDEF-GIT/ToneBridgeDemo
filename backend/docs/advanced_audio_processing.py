"""
ìŒì„± ìë™ ë¶„ì ˆ ë° TextGrid ìƒì„± ì‹œìŠ¤í…œ

ì´ ëª¨ë“ˆì€ í•œêµ­ì–´ ìŒì„±ì˜ ìë™ ìŒì ˆ ë¶„ì ˆê³¼ TextGrid ìƒì„±ì„ ìœ„í•œ
ê³ ê¸‰ ìŒì„± ì²˜ë¦¬ ì‹œìŠ¤í…œì„ êµ¬í˜„í•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
1. ë‹¤ì¤‘ íŠ¹ì§• ê¸°ë°˜ ìŒì ˆ ë¶„ì ˆ (ì—ë„ˆì§€, ìŠ¤í™íŠ¸ëŸ´, í”¼ì¹˜)
2. ë™ì  í”„ë¡œê·¸ë˜ë°ì„ í†µí•œ ìµœì  ê²½ê³„ íƒì§€
3. ìë™ TextGrid ìƒì„± ë° ìµœì í™”
4. STT í†µí•©ì„ í†µí•œ ì™„ì „ ìë™í™”
"""

import numpy as np
import parselmouth
from parselmouth.praat import call
import librosa
import scipy.signal
from dataclasses import dataclass, field
from typing import List, Tuple, Optional, Dict
from pathlib import Path
import json

@dataclass
class Syllable:
    """ìŒì ˆ ì •ë³´ í´ë˜ìŠ¤"""
    start_time: float
    end_time: float
    text: str = ""
    pitch_mean: float = 0.0
    intensity_mean: float = 0.0
    confidence: float = 0.0
    phonemes: List[str] = field(default_factory=list)
    
class SyllableSegmentation:
    """
    ìŒì ˆ ìë™ ë¶„ì ˆ ì‹œìŠ¤í…œ
    """
    
    def __init__(self):
        self.sample_rate = None
        self.audio = None
        self.sound = None
        
    def segment_syllables(self, audio_file: str, method: str = 'energy_based') -> List[Syllable]:
        """
        ìŒì ˆ ë¶„ì ˆ ë©”ì¸ í•¨ìˆ˜
        
        Parameters:
        -----------
        audio_file : str
            ì…ë ¥ ì˜¤ë””ì˜¤ íŒŒì¼
        method : str
            ë¶„ì ˆ ë°©ë²• ('energy_based', 'spectral_based', 'hybrid')
        
        Returns:
        --------
        List[Syllable] : ë¶„ì ˆëœ ìŒì ˆ ë¦¬ìŠ¤íŠ¸
        """
        # ì˜¤ë””ì˜¤ ë¡œë“œ
        self.sound = parselmouth.Sound(audio_file)
        self.audio, self.sample_rate = librosa.load(audio_file, sr=None)
        
        if method == 'energy_based':
            return self._energy_based_segmentation()
        elif method == 'spectral_based':
            return self._spectral_based_segmentation()
        elif method == 'hybrid':
            return self._hybrid_segmentation()
        else:
            raise ValueError(f"Unknown method: {method}")
    
    def _energy_based_segmentation(self) -> List[Syllable]:
        """
        ì—ë„ˆì§€ ê¸°ë°˜ ìŒì ˆ ë¶„ì ˆ
        """
        # 1. ì—ë„ˆì§€ ì—”ë²¨ë¡œí”„ ê³„ì‚°
        intensity = self.sound.to_intensity(time_step=0.01)
        times = intensity.xs()
        values = intensity.values[0]
        
        # 2. ì—ë„ˆì§€ í”¼í¬ì™€ ë°¸ë¦¬ ì°¾ê¸°
        peaks, valleys = self._find_peaks_and_valleys(values)
        
        # 3. ìŒì ˆ ê²½ê³„ ê²°ì •
        boundaries = self._determine_boundaries_from_energy(
            times, values, peaks, valleys
        )
        
        # 4. ìŒì ˆ ê°ì²´ ìƒì„±
        syllables = []
        for i in range(len(boundaries) - 1):
            syllable = Syllable(
                start_time=boundaries[i],
                end_time=boundaries[i + 1],
                intensity_mean=np.mean(values[
                    int(boundaries[i] * 100):int(boundaries[i + 1] * 100)
                ])
            )
            syllables.append(syllable)
        
        return syllables
    
    def _spectral_based_segmentation(self) -> List[Syllable]:
        """
        ìŠ¤í™íŠ¸ëŸ´ íŠ¹ì§• ê¸°ë°˜ ìŒì ˆ ë¶„ì ˆ
        """
        # 1. ìŠ¤í™íŠ¸ëŸ´ íŠ¹ì§• ì¶”ì¶œ
        spectral_flux = self._compute_spectral_flux()
        spectral_centroid = librosa.feature.spectral_centroid(
            y=self.audio, sr=self.sample_rate
        )[0]
        
        # 2. ë³€í™”ì  ê²€ì¶œ
        change_points = self._detect_spectral_changes(
            spectral_flux, spectral_centroid
        )
        
        # 3. ìŒì ˆ ê²½ê³„ë¡œ ë³€í™˜
        boundaries = change_points / self.sample_rate
        
        # 4. ìŒì ˆ ìƒì„±
        syllables = []
        for i in range(len(boundaries) - 1):
            syllables.append(
                Syllable(
                    start_time=boundaries[i],
                    end_time=boundaries[i + 1]
                )
            )
        
        return syllables
    
    def _hybrid_segmentation(self) -> List[Syllable]:
        """
        í•˜ì´ë¸Œë¦¬ë“œ ìŒì ˆ ë¶„ì ˆ (ì—ë„ˆì§€ + ìŠ¤í™íŠ¸ëŸ´ + í”¼ì¹˜)
        """
        # 1. ë‹¤ì¤‘ íŠ¹ì§• ì¶”ì¶œ
        features = self._extract_multiple_features()
        
        # 2. íŠ¹ì§• ìœµí•©
        combined_score = self._fuse_features(features)
        
        # 3. ë™ì  í”„ë¡œê·¸ë˜ë°ìœ¼ë¡œ ìµœì  ê²½ê³„ ì°¾ê¸°
        boundaries = self._dynamic_programming_segmentation(combined_score)
        
        # 4. ìŒì ˆ ì •ë³´ ì¶”ì¶œ
        syllables = self._extract_syllable_info(boundaries)
        
        return syllables
    
    def _find_peaks_and_valleys(self, signal: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """
        ì‹ í˜¸ì—ì„œ í”¼í¬ì™€ ë°¸ë¦¬ ì°¾ê¸°
        """
        # ìŠ¤ë¬´ë”©
        smoothed = scipy.signal.savgol_filter(signal, 11, 3)
        
        # í”¼í¬ ì°¾ê¸°
        peaks, _ = scipy.signal.find_peaks(
            smoothed, 
            prominence=np.std(smoothed) * 0.3,
            distance=20  # ìµœì†Œ 200ms ê°„ê²©
        )
        
        # ë°¸ë¦¬ ì°¾ê¸° (ë°˜ì „ ì‹ í˜¸ì˜ í”¼í¬)
        valleys, _ = scipy.signal.find_peaks(
            -smoothed,
            prominence=np.std(smoothed) * 0.2,
            distance=10
        )
        
        return peaks, valleys
    
    def _determine_boundaries_from_energy(self, times, values, peaks, valleys):
        """
        ì—ë„ˆì§€ í”¼í¬ì™€ ë°¸ë¦¬ë¡œë¶€í„° ìŒì ˆ ê²½ê³„ ê²°ì •
        """
        boundaries = [0.0]  # ì‹œì‘ì 
        
        # ë°¸ë¦¬ë¥¼ ê²½ê³„ë¡œ ì‚¬ìš©
        for valley_idx in valleys:
            # ë°¸ë¦¬ê°€ ë‘ í”¼í¬ ì‚¬ì´ì— ìˆëŠ”ì§€ í™•ì¸
            if valley_idx < len(times):
                time = times[valley_idx]
                
                # ë„ˆë¬´ ì§§ì€ ìŒì ˆ ë°©ì§€ (ìµœì†Œ 50ms)
                if len(boundaries) == 0 or time - boundaries[-1] > 0.05:
                    boundaries.append(time)
        
        boundaries.append(times[-1])  # ëì 
        
        return boundaries
    
    def _compute_spectral_flux(self):
        """
        ìŠ¤í™íŠ¸ëŸ´ í”ŒëŸ­ìŠ¤ ê³„ì‚°
        """
        stft = librosa.stft(self.audio)
        magnitude = np.abs(stft)
        
        # ìŠ¤í™íŠ¸ëŸ´ í”ŒëŸ­ìŠ¤
        flux = np.sum(np.diff(magnitude, axis=1) ** 2, axis=0)
        flux = np.pad(flux, (1, 0), mode='constant', constant_values=0)
        
        return flux
    
    def _detect_spectral_changes(self, flux, centroid):
        """
        ìŠ¤í™íŠ¸ëŸ´ ë³€í™”ì  ê²€ì¶œ
        """
        # ì •ê·œí™”
        flux_norm = (flux - np.mean(flux)) / np.std(flux)
        centroid_norm = (centroid - np.mean(centroid)) / np.std(centroid)
        
        # ë³€í™” ìŠ¤ì½”ì–´
        change_score = np.abs(np.diff(flux_norm)) + np.abs(np.diff(centroid_norm))
        
        # í”¼í¬ ê²€ì¶œ
        peaks, _ = scipy.signal.find_peaks(
            change_score,
            prominence=1.0,
            distance=int(0.05 * self.sample_rate)  # ìµœì†Œ 50ms
        )
        
        return peaks
    
    def _extract_multiple_features(self):
        """
        ë‹¤ì¤‘ íŠ¹ì§• ì¶”ì¶œ
        """
        features = {}
        
        # 1. ì—ë„ˆì§€
        features['energy'] = librosa.feature.rms(
            y=self.audio, 
            frame_length=2048, 
            hop_length=512
        )[0]
        
        # 2. ì˜êµì°¨ìœ¨
        features['zcr'] = librosa.feature.zero_crossing_rate(
            self.audio,
            frame_length=2048,
            hop_length=512
        )[0]
        
        # 3. ìŠ¤í™íŠ¸ëŸ´ ì¤‘ì‹¬
        features['spectral_centroid'] = librosa.feature.spectral_centroid(
            y=self.audio,
            sr=self.sample_rate
        )[0]
        
        # 4. í”¼ì¹˜ (Praat)
        pitch = self.sound.to_pitch(time_step=0.01)
        features['pitch'] = pitch.selected_array['frequency']
        
        # 5. í¬ë¨¼íŠ¸
        formant = self.sound.to_formant_burg()
        f1_values = []
        for i in range(formant.n_frames):
            time = formant.get_time_from_frame_number(i + 1)
            f1 = formant.get_value_at_time(1, time)
            f1_values.append(f1 if f1 else 0)
        features['formant'] = np.array(f1_values)
        
        return features
    
    def _fuse_features(self, features):
        """
        íŠ¹ì§• ìœµí•©
        """
        # ëª¨ë“  íŠ¹ì§•ì„ ë™ì¼í•œ ì‹œê°„ í•´ìƒë„ë¡œ ë¦¬ìƒ˜í”Œë§
        target_length = min(len(v) for v in features.values())
        
        fused = np.zeros(target_length)
        weights = {
            'energy': 0.3,
            'zcr': 0.1,
            'spectral_centroid': 0.2,
            'pitch': 0.25,
            'formant': 0.15
        }
        
        for name, feature in features.items():
            # ë¦¬ìƒ˜í”Œë§
            if len(feature) != target_length:
                feature = scipy.signal.resample(feature, target_length)
            
            # ì •ê·œí™”
            if np.std(feature) > 0:
                feature = (feature - np.mean(feature)) / np.std(feature)
            
            # ê°€ì¤‘ í•©
            fused += weights.get(name, 0.1) * np.abs(np.diff(np.pad(feature, (1, 0))))
        
        return fused
    
    def _dynamic_programming_segmentation(self, score, min_duration=0.05, max_duration=0.5):
        """
        ë™ì  í”„ë¡œê·¸ë˜ë°ì„ ì´ìš©í•œ ìµœì  ë¶„ì ˆ
        """
        n = len(score)
        hop_length = 512
        time_per_frame = hop_length / self.sample_rate
        
        min_frames = int(min_duration / time_per_frame)
        max_frames = int(max_duration / time_per_frame)
        
        # DP í…Œì´ë¸”
        dp = np.full(n + 1, np.inf)
        dp[0] = 0
        parent = np.zeros(n + 1, dtype=int)
        
        # DP ì‹¤í–‰
        for i in range(n):
            if dp[i] == np.inf:
                continue
                
            for j in range(i + min_frames, min(i + max_frames + 1, n + 1)):
                # ì„¸ê·¸ë¨¼íŠ¸ ë¹„ìš©
                segment_cost = -np.sum(score[i:j])
                
                # ê¸¸ì´ í˜ë„í‹°
                length = j - i
                optimal_length = (min_frames + max_frames) / 2
                length_penalty = abs(length - optimal_length) * 0.1
                
                total_cost = dp[i] + segment_cost + length_penalty
                
                if total_cost < dp[j]:
                    dp[j] = total_cost
                    parent[j] = i
        
        # ë°±íŠ¸ë˜í‚¹
        boundaries = []
        i = n
        while i > 0:
            boundaries.append(i * time_per_frame)
            i = parent[i]
        boundaries.append(0)
        
        return list(reversed(boundaries))
    
    def _extract_syllable_info(self, boundaries):
        """
        ê²½ê³„ë¡œë¶€í„° ìŒì ˆ ì •ë³´ ì¶”ì¶œ
        """
        syllables = []
        
        for i in range(len(boundaries) - 1):
            start = boundaries[i]
            end = boundaries[i + 1]
            
            # êµ¬ê°„ ë‚´ íŠ¹ì§• ì¶”ì¶œ
            start_frame = int(start * self.sample_rate)
            end_frame = int(end * self.sample_rate)
            
            if end_frame > start_frame:
                segment = self.audio[start_frame:end_frame]
                
                # í”¼ì¹˜ í‰ê· 
                pitch = self.sound.to_pitch(time_step=0.01)
                pitch_values = []
                for t in np.linspace(start, end, 10):
                    p = pitch.get_value_at_time(t)
                    if p:
                        pitch_values.append(p)
                
                pitch_mean = np.mean(pitch_values) if pitch_values else 0
                
                # ê°•ë„ í‰ê· 
                intensity = self.sound.to_intensity()
                intensity_values = []
                for t in np.linspace(start, end, 10):
                    intensity_values.append(
                        call(intensity, "Get value at time", t, "cubic")
                    )
                intensity_mean = np.mean(intensity_values)
                
                syllables.append(
                    Syllable(
                        start_time=start,
                        end_time=end,
                        pitch_mean=pitch_mean,
                        intensity_mean=intensity_mean,
                        confidence=0.8  # ì„ì‹œ ì‹ ë¢°ë„
                    )
                )
        
        return syllables


class TextGridManager:
    """
    TextGrid íŒŒì¼ ìƒì„± ë° ê´€ë¦¬
    """
    
    def __init__(self):
        self.textgrid = None
        self.tiers = {}
        
    def create_textgrid(self, duration: float, tiers_config: Dict = None):
        """
        ìƒˆ TextGrid ìƒì„±
        """
        if tiers_config is None:
            tiers_config = {
                'syllables': 'IntervalTier',
                'phonemes': 'IntervalTier', 
                'words': 'IntervalTier'
            }
        
        # TextGrid ì´ˆê¸°í™”
        self.textgrid = {
            'xmin': 0.0,
            'xmax': duration,
            'tiers': []
        }
        
        # ê° tier ìƒì„±
        for tier_name, tier_type in tiers_config.items():
            tier = {
                'name': tier_name,
                'class': tier_type,
                'xmin': 0.0,
                'xmax': duration,
                'intervals': []
            }
            self.textgrid['tiers'].append(tier)
            self.tiers[tier_name] = len(self.textgrid['tiers']) - 1
    
    def add_syllable_intervals(self, syllables: List[Syllable]):
        """
        ìŒì ˆ êµ¬ê°„ì„ TextGridì— ì¶”ê°€
        """
        if 'syllables' not in self.tiers:
            raise ValueError("syllables tier not found")
        
        tier_idx = self.tiers['syllables']
        intervals = []
        
        for syllable in syllables:
            intervals.append({
                'xmin': syllable.start_time,
                'xmax': syllable.end_time,
                'text': syllable.text
            })
        
        self.textgrid['tiers'][tier_idx]['intervals'] = intervals
    
    def save_textgrid(self, output_path: str):
        """
        TextGridë¥¼ íŒŒì¼ë¡œ ì €ì¥
        """
        content = self._format_textgrid()
        
        with open(output_path, 'w', encoding='utf-16') as f:
            f.write(content)
    
    def _format_textgrid(self) -> str:
        """
        TextGrid í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ…
        """
        content = f'''File type = "ooTextFile"
Object class = "TextGrid"

xmin = {self.textgrid['xmin']} 
xmax = {self.textgrid['xmax']} 
tiers? <exists> 
size = {len(self.textgrid['tiers'])} 
item []: 
'''
        
        for i, tier in enumerate(self.textgrid['tiers']):
            content += f'''    item [{i+1}]:
        class = "{tier['class']}" 
        name = "{tier['name']}" 
        xmin = {tier['xmin']} 
        xmax = {tier['xmax']} 
        intervals: size = {len(tier['intervals'])} 
'''
            
            for j, interval in enumerate(tier['intervals']):
                content += f'''        intervals [{j+1}]:
            xmin = {interval['xmin']} 
            xmax = {interval['xmax']} 
            text = "{interval['text']}" 
'''
        
        return content


class STTProcessor:
    """
    ìŒì„±-í…ìŠ¤íŠ¸ ë³€í™˜ ì²˜ë¦¬ê¸°
    """
    
    def __init__(self, model_type: str = 'whisper'):
        self.model_type = model_type
        self.model = None
        
    def transcribe_audio(self, audio_file: str, language: str = 'ko') -> str:
        """
        ì˜¤ë””ì˜¤ íŒŒì¼ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        """
        if self.model_type == 'whisper':
            return self._whisper_transcribe(audio_file, language)
        else:
            raise ValueError(f"Unsupported model type: {self.model_type}")
    
    def _whisper_transcribe(self, audio_file: str, language: str) -> str:
        """
        OpenAI Whisperë¥¼ ì‚¬ìš©í•œ ìŒì„± ì¸ì‹
        """
        try:
            import whisper
            
            if self.model is None:
                self.model = whisper.load_model("base")
            
            result = self.model.transcribe(audio_file, language=language)
            return result["text"].strip()
            
        except ImportError:
            raise ImportError("whisper library not installed. Run: pip install openai-whisper")
    
    def segment_with_timestamps(self, audio_file: str, language: str = 'ko') -> List[Dict]:
        """
        íƒ€ì„ìŠ¤íƒ¬í”„ì™€ í•¨ê»˜ ìŒì„± ì¸ì‹
        """
        try:
            import whisper
            
            if self.model is None:
                self.model = whisper.load_model("base")
            
            result = self.model.transcribe(audio_file, language=language, word_timestamps=True)
            
            segments = []
            for segment in result["segments"]:
                segments.append({
                    'start': segment['start'],
                    'end': segment['end'], 
                    'text': segment['text'].strip()
                })
            
            return segments
            
        except ImportError:
            raise ImportError("whisper library not installed")


class AutomatedAudioProcessor:
    """
    ì™„ì „ ìë™í™”ëœ ì˜¤ë””ì˜¤ ì²˜ë¦¬ ì‹œìŠ¤í…œ
    """
    
    def __init__(self):
        self.segmenter = SyllableSegmentation()
        self.textgrid_manager = TextGridManager()
        self.stt_processor = STTProcessor()
    
    def process_audio_file(self, audio_file: str, output_dir: str = None) -> Dict:
        """
        ì˜¤ë””ì˜¤ íŒŒì¼ ì™„ì „ ìë™ ì²˜ë¦¬
        
        Returns:
        --------
        Dict : ì²˜ë¦¬ ê²°ê³¼
            - transcription: ìŒì„± ì¸ì‹ ê²°ê³¼
            - syllables: ë¶„ì ˆëœ ìŒì ˆ ë¦¬ìŠ¤íŠ¸
            - textgrid_path: ìƒì„±ëœ TextGrid íŒŒì¼ ê²½ë¡œ
        """
        if output_dir is None:
            output_dir = Path(audio_file).parent
        
        audio_path = Path(audio_file)
        base_name = audio_path.stem
        
        # 1. ìŒì„± ì¸ì‹
        print("ğŸ¤ ìŒì„± ì¸ì‹ ì‹œì‘...")
        transcription = self.stt_processor.transcribe_audio(audio_file)
        print(f"ğŸ“ ì¸ì‹ ê²°ê³¼: {transcription}")
        
        # 2. ìŒì ˆ ë¶„ì ˆ
        print("âœ‚ï¸ ìŒì ˆ ë¶„ì ˆ ì‹œì‘...")
        syllables = self.segmenter.segment_syllables(audio_file, method='hybrid')
        
        # 3. í…ìŠ¤íŠ¸ì™€ ìŒì ˆ ë§¤ì¹­
        print("ğŸ”— í…ìŠ¤íŠ¸-ìŒì ˆ ë§¤ì¹­...")
        syllables = self._match_text_to_syllables(transcription, syllables)
        
        # 4. TextGrid ìƒì„±
        print("ğŸ“‹ TextGrid ìƒì„±...")
        duration = self.segmenter.sound.duration
        self.textgrid_manager.create_textgrid(duration)
        self.textgrid_manager.add_syllable_intervals(syllables)
        
        # 5. íŒŒì¼ ì €ì¥
        textgrid_path = Path(output_dir) / f"{base_name}.TextGrid"
        self.textgrid_manager.save_textgrid(str(textgrid_path))
        
        print(f"âœ… ì²˜ë¦¬ ì™„ë£Œ: {textgrid_path}")
        
        return {
            'transcription': transcription,
            'syllables': [
                {
                    'text': s.text,
                    'start': s.start_time,
                    'end': s.end_time,
                    'pitch_mean': s.pitch_mean,
                    'intensity_mean': s.intensity_mean
                }
                for s in syllables
            ],
            'textgrid_path': str(textgrid_path)
        }
    
    def _match_text_to_syllables(self, text: str, syllables: List[Syllable]) -> List[Syllable]:
        """
        ì¸ì‹ëœ í…ìŠ¤íŠ¸ë¥¼ ìŒì ˆ ë¶„ì ˆ ê²°ê³¼ì™€ ë§¤ì¹­
        """
        # í•œêµ­ì–´ ìŒì ˆ ë¶„ë¦¬
        clean_text = ''.join(c for c in text if c.strip() and not c.isascii())
        korean_syllables = list(clean_text)
        
        print(f"ğŸ¯ ì¸ì‹ëœ ìŒì ˆ: {korean_syllables} ({len(korean_syllables)}ê°œ)")
        print(f"ğŸ¯ ë¶„ì ˆëœ êµ¬ê°„: {len(syllables)}ê°œ")
        
        # ë§¤ì¹­ ì „ëµ: ê¸¸ì´ê°€ ë‹¤ë¥´ë©´ ê· ë“± ë¶„ë°°
        if len(korean_syllables) != len(syllables):
            print("âš ï¸ ìŒì ˆ ìˆ˜ ë¶ˆì¼ì¹˜ - ê· ë“± ë¶„ë°° ì ìš©")
            
            # ìƒˆë¡œìš´ ìŒì ˆ ë¦¬ìŠ¤íŠ¸ ìƒì„±
            new_syllables = []
            total_duration = syllables[-1].end_time - syllables[0].start_time
            syllable_duration = total_duration / len(korean_syllables)
            
            for i, syllable_text in enumerate(korean_syllables):
                start_time = syllables[0].start_time + i * syllable_duration
                end_time = start_time + syllable_duration
                
                new_syllables.append(
                    Syllable(
                        start_time=start_time,
                        end_time=end_time,
                        text=syllable_text,
                        confidence=0.7
                    )
                )
            
            return new_syllables
        
        # ê¸¸ì´ê°€ ê°™ìœ¼ë©´ ì§ì ‘ ë§¤ì¹­
        for i, syllable in enumerate(syllables):
            if i < len(korean_syllables):
                syllable.text = korean_syllables[i]
                syllable.confidence = 0.9
        
        return syllables