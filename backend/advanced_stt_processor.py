"""
ToneBridge ê³ ê¸‰ STT ì²˜ë¦¬ ì‹œìŠ¤í…œ
ë‹¤ì¤‘ STT ì—”ì§„ ì§€ì› ë° í•œêµ­ì–´ íŠ¹í™” ìŒì ˆ ì •ë ¬
"""

import numpy as np
import parselmouth as pm
from pathlib import Path
from typing import List, Dict, Optional, Tuple
from dataclasses import dataclass, field
import json
import os
import sys
import requests

@dataclass
class TranscriptionResult:
    """ì „ì‚¬ ê²°ê³¼ í´ë˜ìŠ¤"""
    text: str
    language: str
    confidence: float
    words: List[Dict] = field(default_factory=list)  # [{'word': str, 'start': float, 'end': float, 'confidence': float}]
    segments: List[Dict] = field(default_factory=list)  # ë¬¸ì¥/êµ¬ ë‹¨ìœ„ ì„¸ê·¸ë¨¼íŠ¸
    engine: str = "whisper"

@dataclass
class SyllableAlignment:
    """ìŒì ˆ ì •ë ¬ ê²°ê³¼"""
    syllable: str
    start_time: float
    end_time: float
    confidence: float
    word_context: str = ""
    phonetic_features: Dict = field(default_factory=dict)

class UniversalSTT:
    """
    ë‹¤ì¤‘ STT ì—”ì§„ í†µí•© í´ë˜ìŠ¤
    """
    
    def __init__(self, engine: str = 'whisper', **kwargs):
        """
        Parameters:
        -----------
        engine : str
            STT ì—”ì§„ ì„ íƒ ('whisper', 'google', 'azure', 'naver_clova', 'local_fallback')
        """
        self.engine = engine
        self.model = None
        self.config = kwargs
        
        # ê° ì—”ì§„ì˜ ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
        self.available_engines = self._check_available_engines()
        
        # ì„ íƒëœ ì—”ì§„ ì´ˆê¸°í™”
        if engine in self.available_engines:
            self._initialize_engine(engine, **kwargs)
        else:
            print(f"âš ï¸ {engine} ì‚¬ìš© ë¶ˆê°€, fallback ëª¨ë“œë¡œ ì „í™˜")
            self.engine = 'local_fallback'
    
    def _check_available_engines(self) -> List[str]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ STT ì—”ì§„ í™•ì¸"""
        available = ['local_fallback']  # í•­ìƒ ì‚¬ìš© ê°€ëŠ¥
        
        # Whisper í™•ì¸
        try:
            import whisper
            available.append('whisper')
            print("âœ… Whisper ì‚¬ìš© ê°€ëŠ¥")
        except ImportError:
            print("âŒ Whisper ë¯¸ì„¤ì¹˜")
        
        # Google Cloud STT í™•ì¸
        try:
            from google.cloud import speech_v1
            available.append('google')
            print("âœ… Google Cloud STT ì‚¬ìš© ê°€ëŠ¥")
        except ImportError:
            print("âŒ Google Cloud STT ë¯¸ì„¤ì¹˜")
        
        # Azure í™•ì¸
        try:
            import azure.cognitiveservices.speech as speechsdk
            available.append('azure')
            print("âœ… Azure Speech Services ì‚¬ìš© ê°€ëŠ¥")
        except ImportError:
            print("âŒ Azure Speech Services ë¯¸ì„¤ì¹˜")
        
        # Naver CLOVAëŠ” API í‚¤ê°€ ìˆìœ¼ë©´ ì‚¬ìš© ê°€ëŠ¥
        if self.config.get('naver_client_id') and self.config.get('naver_client_secret'):
            available.append('naver_clova')
            print("âœ… Naver CLOVA STT ì‚¬ìš© ê°€ëŠ¥")
        
        return available
    
    def _initialize_engine(self, engine: str, **kwargs):
        """ì—”ì§„ ì´ˆê¸°í™”"""
        if engine == 'whisper':
            self._init_whisper(**kwargs)
        elif engine == 'google':
            self._init_google(**kwargs)
        elif engine == 'azure':
            self._init_azure(**kwargs)
        elif engine == 'naver_clova':
            self._init_naver_clova(**kwargs)
        elif engine == 'local_fallback':
            self._init_local_fallback()
    
    def _init_whisper(self, model_size: str = 'small', **kwargs):
        """OpenAI Whisper ì´ˆê¸°í™” (ì •ë°€ë„ ê°œì„ ì„ ìœ„í•´ small ëª¨ë¸ ì‚¬ìš©)"""
        try:
            import whisper
            # í•œêµ­ì–´ ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•´ ë” í° ëª¨ë¸ ì‚¬ìš©
            self.model = whisper.load_model(model_size)
            print(f"ğŸ¤ Whisper {model_size} ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ Whisper ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.engine = 'local_fallback'
    
    def _init_google(self, credentials_path: str = None, **kwargs):
        """Google Cloud Speech-to-Text ì´ˆê¸°í™”"""
        try:
            from google.cloud import speech_v1
            
            if credentials_path and os.path.exists(credentials_path):
                os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = credentials_path
            
            self.client = speech_v1.SpeechClient()
            print("ğŸ¤ Google Cloud STT ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ Google STT ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.engine = 'local_fallback'
    
    def _init_azure(self, subscription_key: str = None, region: str = None, **kwargs):
        """Azure Speech Services ì´ˆê¸°í™”"""
        try:
            import azure.cognitiveservices.speech as speechsdk
            
            if not subscription_key or not region:
                raise ValueError("Azure ì„¤ì • ì •ë³´ ë¶€ì¡±")
            
            speech_config = speechsdk.SpeechConfig(
                subscription=subscription_key,
                region=region
            )
            self.speech_config = speech_config
            print("ğŸ¤ Azure Speech Services ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ Azure STT ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.engine = 'local_fallback'
    
    def _init_naver_clova(self, naver_client_id: str = None, naver_client_secret: str = None, **kwargs):
        """Naver CLOVA Speech ì´ˆê¸°í™”"""
        try:
            if not naver_client_id or not naver_client_secret:
                raise ValueError("Naver CLOVA ì„¤ì • ì •ë³´ ë¶€ì¡±")
            
            self.clova_config = {
                'client_id': naver_client_id,
                'client_secret': naver_client_secret,
                'url': 'https://naveropenapi.apigw.ntruss.com/recog/v1/stt'
            }
            print("ğŸ¤ Naver CLOVA STT ì´ˆê¸°í™” ì™„ë£Œ")
        except Exception as e:
            print(f"âŒ Naver CLOVA STT ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.engine = 'local_fallback'
    
    def _init_local_fallback(self):
        """ë¡œì»¬ fallback ëª¨ë“œ ì´ˆê¸°í™”"""
        print("ğŸ¤ ë¡œì»¬ fallback ëª¨ë“œ í™œì„±í™”")
    
    def transcribe(self, audio_file: str, language: str = 'ko', 
                  return_timestamps: bool = True) -> TranscriptionResult:
        """
        ìŒì„± íŒŒì¼ ì „ì‚¬
        
        Parameters:
        -----------
        audio_file : str
            ì…ë ¥ ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ
        language : str
            ì–¸ì–´ ì½”ë“œ (ì˜ˆ: 'ko', 'en')
        return_timestamps : bool
            íƒ€ì„ìŠ¤íƒ¬í”„ ë°˜í™˜ ì—¬ë¶€
        
        Returns:
        --------
        TranscriptionResult : ì „ì‚¬ ê²°ê³¼
        """
        print(f"ğŸ¤ {self.engine} ì—”ì§„ìœ¼ë¡œ ìŒì„± ì¸ì‹ ì‹œì‘...")
        
        if self.engine == 'whisper':
            result = self._transcribe_whisper(audio_file, language, return_timestamps)
        elif self.engine == 'google':
            result = self._transcribe_google(audio_file, language, return_timestamps)
        elif self.engine == 'azure':
            result = self._transcribe_azure(audio_file, language, return_timestamps)
        elif self.engine == 'naver_clova':
            result = self._transcribe_naver_clova(audio_file, language)
        else:
            result = self._transcribe_local_fallback(audio_file)
        
        # ğŸ” STT ê²°ê³¼ ë””ë²„ê¹… ì •ë³´ ì¶œë ¥
        print(f"ğŸ“ STT í…ìŠ¤íŠ¸ ê²°ê³¼: '{result.text}'")
        if result.words:
            print(f"ğŸ• Word-level íƒ€ì„ìŠ¤íƒ¬í”„ ({len(result.words)}ê°œ):")
            for i, word in enumerate(result.words):
                if hasattr(word, 'word'):
                    word_text = word.word
                    start_time = word.start
                    end_time = word.end
                elif isinstance(word, dict):
                    word_text = word.get('word', '')
                    start_time = word.get('start', 0.0)
                    end_time = word.get('end', 0.0)
                else:
                    word_text = str(word)
                    start_time = 0.0
                    end_time = 0.0
                print(f"  {i+1:2d}. '{word_text}' [{start_time:.3f}s ~ {end_time:.3f}s] (ì§€ì†: {end_time-start_time:.3f}s)")
        else:
            print("âŒ Word-level íƒ€ì„ìŠ¤íƒ¬í”„ ì—†ìŒ")
        
        return result
    
    def _transcribe_whisper(self, audio_file: str, language: str = 'ko',
                           return_timestamps: bool = True) -> TranscriptionResult:
        """Whisperë¡œ ì „ì‚¬"""
        try:
            # ì „ì‚¬ ì˜µì…˜ ì„¤ì •
            options = {
                'word_timestamps': return_timestamps,
                'verbose': False,
                'language': language
            }
            
            # ì „ì‚¬ ì‹¤í–‰
            result = self.model.transcribe(audio_file, **options)
            
            # ë‹¨ì–´ íƒ€ì„ìŠ¤íƒ¬í”„ ì¶”ì¶œ
            words = []
            if return_timestamps and 'segments' in result:
                for segment in result['segments']:
                    if 'words' in segment:
                        for word_info in segment['words']:
                            words.append({
                                'word': word_info['word'].strip(),
                                'start': word_info['start'],
                                'end': word_info['end'],
                                'confidence': word_info.get('probability', 0.0)
                            })
            
            # ì„¸ê·¸ë¨¼íŠ¸ ì •ë³´
            segments = []
            if 'segments' in result:
                for segment in result['segments']:
                    segments.append({
                        'id': segment['id'],
                        'text': segment['text'].strip(),
                        'start': segment['start'],
                        'end': segment['end'],
                        'confidence': segment.get('avg_logprob', 0.0)
                    })
            
            # í•œêµ­ì–´ í…ìŠ¤íŠ¸ í•„í„°ë§
            text = result['text'].strip()
            if language == 'ko':
                text = self._filter_korean_text(text)
            
            confidence = np.mean([s['confidence'] for s in segments]) if segments else 0.0
            
            return TranscriptionResult(
                text=text,
                language=result.get('language', language),
                confidence=confidence,
                words=words,
                segments=segments,
                engine='whisper'
            )
            
        except Exception as e:
            print(f"âŒ Whisper ì „ì‚¬ ì‹¤íŒ¨: {e}")
            return self._transcribe_local_fallback(audio_file)
    
    def _transcribe_google(self, audio_file: str, language: str = 'ko-KR',
                         return_timestamps: bool = True) -> TranscriptionResult:
        """Google Cloud Speech-to-Textë¡œ ì „ì‚¬"""
        try:
            # ì˜¤ë””ì˜¤ íŒŒì¼ ì½ê¸°
            with open(audio_file, 'rb') as f:
                content = f.read()
            
            from google.cloud import speech_v1
            
            # ì˜¤ë””ì˜¤ ì„¤ì •
            audio = speech_v1.RecognitionAudio(content=content)
            
            # ì„¤ì •
            config = speech_v1.RecognitionConfig(
                encoding=speech_v1.RecognitionConfig.AudioEncoding.LINEAR16,
                sample_rate_hertz=16000,
                language_code=language if '-' in language else f"{language}-KR",
                enable_word_time_offsets=return_timestamps,
                enable_automatic_punctuation=True,
                model='latest_long'
            )
            
            # ì „ì‚¬ ì‹¤í–‰
            response = self.client.recognize(config=config, audio=audio)
            
            # ê²°ê³¼ íŒŒì‹±
            text = ""
            words = []
            segments = []
            
            for result in response.results:
                text += result.alternatives[0].transcript + " "
                
                segments.append({
                    'text': result.alternatives[0].transcript,
                    'confidence': result.alternatives[0].confidence,
                    'start': 0,  # Google APIëŠ” ì„¸ê·¸ë¨¼íŠ¸ íƒ€ì„ìŠ¤íƒ¬í”„ ì œê³µ ì•ˆí•¨
                    'end': 0
                })
                
                if return_timestamps and hasattr(result.alternatives[0], 'words'):
                    for word_info in result.alternatives[0].words:
                        words.append({
                            'word': word_info.word,
                            'start': word_info.start_time.total_seconds(),
                            'end': word_info.end_time.total_seconds(),
                            'confidence': result.alternatives[0].confidence
                        })
            
            confidence = np.mean([s['confidence'] for s in segments]) if segments else 0.0
            
            return TranscriptionResult(
                text=text.strip(),
                language=language,
                confidence=confidence,
                words=words,
                segments=segments,
                engine='google'
            )
            
        except Exception as e:
            print(f"âŒ Google STT ì „ì‚¬ ì‹¤íŒ¨: {e}")
            return self._transcribe_local_fallback(audio_file)
    
    def _transcribe_azure(self, audio_file: str, language: str = 'ko-KR',
                         return_timestamps: bool = True) -> TranscriptionResult:
        """Azure Speech Servicesë¡œ ì „ì‚¬"""
        try:
            import azure.cognitiveservices.speech as speechsdk
            
            # ì˜¤ë””ì˜¤ ì„¤ì •
            audio_config = speechsdk.audio.AudioConfig(filename=audio_file)
            
            # ìŒì„± ì¸ì‹ê¸° ìƒì„±
            speech_recognizer = speechsdk.SpeechRecognizer(
                speech_config=self.speech_config,
                audio_config=audio_config,
                language=language if '-' in language else f"{language}-KR"
            )
            
            # ì „ì‚¬ ì‹¤í–‰
            result = speech_recognizer.recognize_once()
            
            if result.reason == speechsdk.ResultReason.RecognizedSpeech:
                return TranscriptionResult(
                    text=result.text,
                    language=language,
                    confidence=1.0,  # AzureëŠ” confidence ì ìˆ˜ ì œê³µ ì•ˆí•¨
                    words=[],  # ë‹¨ì–´ íƒ€ì„ìŠ¤íƒ¬í”„ëŠ” ë” ë³µì¡í•œ ì„¤ì • í•„ìš”
                    segments=[{'text': result.text, 'confidence': 1.0}],
                    engine='azure'
                )
            else:
                raise Exception(f"Azure ì¸ì‹ ì‹¤íŒ¨: {result.reason}")
                
        except Exception as e:
            print(f"âŒ Azure STT ì „ì‚¬ ì‹¤íŒ¨: {e}")
            return self._transcribe_local_fallback(audio_file)
    
    def _transcribe_naver_clova(self, audio_file: str, language: str = 'ko') -> TranscriptionResult:
        """Naver CLOVA Speechë¡œ ì „ì‚¬"""
        try:
            # ì˜¤ë””ì˜¤ íŒŒì¼ ì½ê¸°
            with open(audio_file, 'rb') as f:
                data = f.read()
            
            # í—¤ë” ì„¤ì •
            headers = {
                'X-NCP-APIGW-API-KEY-ID': self.clova_config['client_id'],
                'X-NCP-APIGW-API-KEY': self.clova_config['client_secret'],
                'Content-Type': 'application/octet-stream'
            }
            
            # íŒŒë¼ë¯¸í„°
            params = {
                'lang': language.split('-')[0] if '-' in language else language
            }
            
            # API í˜¸ì¶œ
            response = requests.post(
                self.clova_config['url'],
                headers=headers,
                params=params,
                data=data,
                timeout=30
            )
            
            if response.status_code == 200:
                result = response.json()
                text = result.get('text', '')
                
                return TranscriptionResult(
                    text=text,
                    language=language,
                    confidence=1.0,  # CLOVAëŠ” confidence ì œê³µ ì•ˆí•¨
                    words=[],  # CLOVAëŠ” ë‹¨ì–´ íƒ€ì„ìŠ¤íƒ¬í”„ ì œê³µ ì•ˆí•¨
                    segments=[{'text': text, 'confidence': 1.0}],
                    engine='naver_clova'
                )
            else:
                raise Exception(f"CLOVA API Error: {response.status_code}")
                
        except Exception as e:
            print(f"âŒ Naver CLOVA STT ì „ì‚¬ ì‹¤íŒ¨: {e}")
            return self._transcribe_local_fallback(audio_file)
    
    def _transcribe_local_fallback(self, audio_file: str) -> TranscriptionResult:
        """ë¡œì»¬ fallback ì „ì‚¬ (íŒŒì¼ëª… ê¸°ë°˜)"""
        filename = Path(audio_file).stem
        
        # íŒŒì¼ëª…ì—ì„œ í•œêµ­ì–´ ì¶”ì¶œ
        korean_text = self._filter_korean_text(filename)
        
        if korean_text:
            print(f"ğŸ“ íŒŒì¼ëª… ê¸°ë°˜ ì¶”ì •: {korean_text}")
        else:
            korean_text = "í…ìŠ¤íŠ¸ ì¸ì‹ ì‹¤íŒ¨"
            print("âš ï¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹¤íŒ¨")
        
        return TranscriptionResult(
            text=korean_text,
            language='ko',
            confidence=0.5,  # ë‚®ì€ ì‹ ë¢°ë„
            words=[],
            segments=[{'text': korean_text, 'confidence': 0.5}],
            engine='local_fallback'
        )
    
    def _filter_korean_text(self, text: str) -> str:
        """í•œêµ­ì–´ í…ìŠ¤íŠ¸ë§Œ í•„í„°ë§"""
        korean_chars = ''.join(c for c in text if self._is_korean(c) or c.isspace() or c in '.,!?')
        return korean_chars.strip()
    
    def _is_korean(self, char: str) -> bool:
        """í•œêµ­ì–´ ë¬¸ìì¸ì§€ í™•ì¸"""
        return 0xAC00 <= ord(char) <= 0xD7A3 if len(char) == 1 else False


class KoreanSyllableAligner:
    """
    í•œêµ­ì–´ íŠ¹í™” ìŒì ˆ ì •ë ¬ ì‹œìŠ¤í…œ
    """
    
    def __init__(self):
        self.jamo_dict = self._build_jamo_dictionary()
    
    def _build_jamo_dictionary(self) -> Dict:
        """í•œêµ­ì–´ ìëª¨ ì‚¬ì „ êµ¬ì¶•"""
        # ì´ˆì„±, ì¤‘ì„±, ì¢…ì„± ì •ì˜
        initial = ['ã„±', 'ã„²', 'ã„´', 'ã„·', 'ã„¸', 'ã„¹', 'ã…', 'ã…‚', 'ã…ƒ', 
                  'ã……', 'ã…†', 'ã…‡', 'ã…ˆ', 'ã…‰', 'ã…Š', 'ã…‹', 'ã…Œ', 'ã…', 'ã…']
        medial = ['ã…', 'ã…', 'ã…‘', 'ã…’', 'ã…“', 'ã…”', 'ã…•', 'ã…–', 'ã…—', 'ã…˜',
                 'ã…™', 'ã…š', 'ã…›', 'ã…œ', 'ã…', 'ã…', 'ã…Ÿ', 'ã… ', 'ã…¡', 'ã…¢', 'ã…£']
        final = ['', 'ã„±', 'ã„²', 'ã„³', 'ã„´', 'ã„µ', 'ã„¶', 'ã„·', 'ã„¹', 'ã„º', 'ã„»',
                'ã„¼', 'ã„½', 'ã„¾', 'ã„¿', 'ã…€', 'ã…', 'ã…‚', 'ã…„', 'ã……', 'ã…†', 
                'ã…‡', 'ã…ˆ', 'ã…Š', 'ã…‹', 'ã…Œ', 'ã…', 'ã…']
        
        return {
            'initial': initial,
            'medial': medial,
            'final': final
        }
    
    def decompose_syllable(self, syllable: str) -> Tuple[str, str, str]:
        """ìŒì ˆì„ ìëª¨ë¡œ ë¶„í•´"""
        if not syllable or len(syllable) != 1:
            return ('', '', '')
        
        code = ord(syllable) - 0xAC00
        if code < 0:
            return ('', '', '')
        
        initial_idx = code // (21 * 28)
        medial_idx = (code % (21 * 28)) // 28
        final_idx = code % 28
        
        initial = self.jamo_dict['initial'][initial_idx] if initial_idx < len(self.jamo_dict['initial']) else ''
        medial = self.jamo_dict['medial'][medial_idx] if medial_idx < len(self.jamo_dict['medial']) else ''
        final = self.jamo_dict['final'][final_idx] if final_idx < len(self.jamo_dict['final']) else ''
        
        return (initial, medial, final)
    
    def align_syllables_with_timestamps(self, transcription: TranscriptionResult, 
                                      audio_file: str) -> List[SyllableAlignment]:
        """
        ì „ì‚¬ ê²°ê³¼ë¥¼ ìŒì ˆ ë‹¨ìœ„ë¡œ íƒ€ì„ìŠ¤íƒ¬í”„ì™€ í•¨ê»˜ ì •ë ¬ (ìŒì„± ì‹œì‘ì  ìë™ ê°ì§€)
        """
        print(f"ğŸ¯ ìŒì ˆ ì •ë ¬ ì‹œì‘: {transcription.text}")
        
        # í…ìŠ¤íŠ¸ë¥¼ ìŒì ˆ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
        syllables = list(transcription.text.replace(' ', ''))
        korean_syllables = [s for s in syllables if self._is_korean(s)]
        
        print(f"ğŸ”¤ í•œêµ­ì–´ ìŒì ˆ: {korean_syllables} ({len(korean_syllables)}ê°œ)")
        
        # ë‹¨ì–´ íƒ€ì„ìŠ¤íƒ¬í”„ê°€ ìˆìœ¼ë©´ í™œìš©
        if transcription.words:
            return self._align_with_word_timestamps(korean_syllables, transcription.words, audio_file)
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ê°€ ì—†ìœ¼ë©´ ì˜¤ë””ì˜¤ ê¸¸ì´ ê¸°ë°˜ ê· ë“± ë¶„í• 
        return self._align_with_uniform_distribution(korean_syllables, audio_file)
    
    def _align_with_word_timestamps(self, syllables: List[str], 
                                  words: List[Dict], audio_file: str = None) -> List[SyllableAlignment]:
        """ë‹¨ì–´ íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ í™œìš©í•œ ìŒì ˆ ì •ë ¬ (ì‹¤ì œ ìŒì„± ì‹œì‘ì  ë³´ì •)"""
        print(f"ğŸ”§ Word-level íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ë°˜ ìŒì ˆ ì •ë ¬ ì‹œì‘")
        
        # ğŸ¯ ì‹¤ì œ ìŒì„± ì‹œì‘ì  ê°ì§€ (Voice Activity Detection)
        actual_start = self._detect_voice_start_time(words, audio_file)
        if actual_start > 0:
            print(f"ğŸ¤ ì‹¤ì œ ìŒì„± ì‹œì‘ì  ê°ì§€: {actual_start:.3f}s (ë¬´ìŒ êµ¬ê°„ ì œê±°)")
            # ëª¨ë“  word íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ ì‹¤ì œ ì‹œì‘ì ë§Œí¼ ë³´ì •
            words = self._adjust_word_timestamps(words, actual_start)
        
        alignments = []
        syllable_idx = 0
        
        for word_idx, word_info in enumerate(words):
            # Word êµ¬ì¡° í™•ì¸ ë° ì ì ˆí•œ ì ‘ê·¼
            if hasattr(word_info, 'word'):
                word = word_info.word.strip()
                start_time = word_info.start
                end_time = word_info.end
            elif isinstance(word_info, dict):
                word = word_info.get('word', '').strip()
                start_time = word_info.get('start', 0.0)
                end_time = word_info.get('end', 0.0)
            else:
                print(f"  âŒ ì•Œ ìˆ˜ ì—†ëŠ” word êµ¬ì¡°: {type(word_info)}")
                continue
                
            word_syllables = [s for s in word if self._is_korean(s)]
            
            if not word_syllables:
                print(f"  â© ë‹¨ì–´ {word_idx+1}: '{word}' - í•œêµ­ì–´ ìŒì ˆ ì—†ìŒ, ê±´ë„ˆëœ€")
                continue
            
            # ë‹¨ì–´ ë‚´ ìŒì ˆë“¤ì˜ ì‹œê°„ ê°„ê²© ê³„ì‚°
            word_duration = end_time - start_time
            syllable_duration = word_duration / len(word_syllables)
            
            print(f"  ğŸ“ ë‹¨ì–´ {word_idx+1}: '{word}' [{start_time:.3f}s ~ {end_time:.3f}s]")
            print(f"    ğŸ“Š ìŒì ˆ: {word_syllables} ({len(word_syllables)}ê°œ)")
            print(f"    â±ï¸ ë‹¨ì–´ ì§€ì†ì‹œê°„: {word_duration:.3f}s â†’ ìŒì ˆë‹¹ {syllable_duration:.3f}s")
            
            for i, syllable in enumerate(word_syllables):
                if syllable_idx < len(syllables):
                    syl_start_time = start_time + i * syllable_duration
                    syl_end_time = syl_start_time + syllable_duration
                    
                    print(f"      ğŸ¯ ìŒì ˆ {syllable_idx+1}: '{syllable}' [{syl_start_time:.3f}s ~ {syl_end_time:.3f}s] (ì§€ì†: {syl_end_time-syl_start_time:.3f}s)")
                    
                    # ìëª¨ ë¶„í•´ë¡œ ìŒì„±í•™ì  íŠ¹ì§• ì¶”ì¶œ
                    initial, medial, final = self.decompose_syllable(syllable)
                    
                    alignments.append(SyllableAlignment(
                        syllable=syllable,
                        start_time=syl_start_time,  # ğŸ”§ ìŒì ˆë³„ ì •í™•í•œ ì‹œì‘ ì‹œê°„
                        end_time=syl_end_time,      # ğŸ”§ ìŒì ˆë³„ ì •í™•í•œ ì¢…ë£Œ ì‹œê°„
                        confidence=0.8,  # word_infoì— confidenceê°€ ì—†ì„ ìˆ˜ ìˆìŒ
                        word_context=word,
                        phonetic_features={
                            'initial': initial,
                            'medial': medial,
                            'final': final
                        }
                    ))
                    
                    syllable_idx += 1
        
        return alignments
    
    def _detect_voice_start_time(self, words: List[Dict], audio_file: str = None) -> float:
        """ì‹¤ì œ ìŒì„± ì‹œì‘ ì‹œê°„ ê°ì§€ (ì˜¤ë””ì˜¤ ë¶„ì„ ê¸°ë°˜)"""
        if not words:
            return 0.0
        
        # 1ì°¨: STT word íƒ€ì„ìŠ¤íƒ¬í”„ ê¸°ë°˜ ê°ì§€
        first_word = words[0]
        if hasattr(first_word, 'start'):
            stt_start = first_word.start
        elif isinstance(first_word, dict):
            stt_start = first_word.get('start', 0.0)
        else:
            stt_start = 0.0
        
        # 2ì°¨: STT word ê¸¸ì´ ë¶„ì„ìœ¼ë¡œ ë¬´ìŒ êµ¬ê°„ ê°ì§€ (ë” ì •í™•í•¨)
        first_word = words[0]
        if hasattr(first_word, 'end'):
            first_duration = first_word.end - first_word.start
        elif isinstance(first_word, dict):
            first_duration = first_word.get('end', 0) - first_word.get('start', 0)
        else:
            first_duration = 0
            
        # ì²« ë²ˆì§¸ ë‹¨ì–´ê°€ 1.5ì´ˆ ì´ìƒ ì§€ì†ë˜ë©´ ë¬´ìŒ êµ¬ê°„ í¬í•¨ìœ¼ë¡œ ê°„ì£¼
        if first_duration > 1.5:
            estimated_silence = first_duration * 0.7  # 70%ëŠ” ë¬´ìŒìœ¼ë¡œ ì¶”ì •
            print(f"ğŸ¤ STT ì²« ë‹¨ì–´ ê³¼ë„í•˜ê²Œ ê¸¸ìŒ ({first_duration:.3f}s), ë¬´ìŒ êµ¬ê°„ ì¶”ì •: {estimated_silence:.3f}s")
            return estimated_silence
        
        # ê¸°ì¡´ ë¡œì§: ì²« ë‹¨ì–´ê°€ 0.5ì´ˆ ì´í›„ ì‹œì‘
        if stt_start > 0.5:
            print(f"ğŸ¤ STT ê¸°ë°˜ ë¬´ìŒ êµ¬ê°„ ê°ì§€: {stt_start:.3f}s")
            return stt_start
        
        # 3ì°¨: ì‹¤ì œ ì˜¤ë””ì˜¤ íŒŒì¼ì—ì„œ ìŒì„± ì‹œì‘ì  ê°ì§€ (ë³´ì¡°)
        if audio_file:
            try:
                audio_start = self._detect_audio_voice_start(audio_file)
                if audio_start > 0.1:  # 100ms ì´ìƒ ì°¨ì´ë‚  ë•Œë§Œ ì‚¬ìš©
                    print(f"ğŸ¤ ì˜¤ë””ì˜¤ ë¶„ì„ ê¸°ë°˜ ìŒì„± ì‹œì‘: {audio_start:.3f}s (STT: {stt_start:.3f}s)")
                    return audio_start
            except Exception as e:
                print(f"âš ï¸ ì˜¤ë””ì˜¤ ê¸°ë°˜ ê°ì§€ ì‹¤íŒ¨: {e}, STT ê¸°ì¤€ ì‚¬ìš©")
        
        return stt_start
    
    def _detect_audio_voice_start(self, audio_file: str, 
                                energy_threshold: float = 0.001,
                                silence_duration: float = 0.05) -> float:
        """ì˜¤ë””ì˜¤ íŒŒì¼ì—ì„œ ì‹¤ì œ ìŒì„± ì‹œì‘ì  ê°ì§€"""
        import parselmouth as pm
        
        try:
            # ì˜¤ë””ì˜¤ ë¡œë“œ
            sound = pm.Sound(audio_file)
            
            # ì—ë„ˆì§€ ë¶„ì„ (RMS)
            window_size = 0.05  # 50ms ìœˆë„ìš°
            hop_size = 0.01     # 10ms ìŠ¤í…
            
            duration = sound.get_total_duration()
            time_points = []
            energy_values = []
            
            current_time = 0
            while current_time + window_size <= duration:
                # í•´ë‹¹ êµ¬ê°„ì˜ ì—ë„ˆì§€ ê³„ì‚°
                start_sample = int(current_time * sound.sampling_frequency)
                end_sample = int((current_time + window_size) * sound.sampling_frequency)
                
                if end_sample <= len(sound.values):
                    window_samples = sound.values[start_sample:end_sample]
                    rms_energy = (sum(sample**2 for sample in window_samples) / len(window_samples))**0.5
                    
                    time_points.append(current_time)
                    energy_values.append(rms_energy)
                
                current_time += hop_size
            
            # ìŒì„± ì‹œì‘ì  ì°¾ê¸°: energy_thresholdë¥¼ ì´ˆê³¼í•˜ëŠ” ì²« ì§€ì 
            for i, energy in enumerate(energy_values):
                if energy > energy_threshold:
                    voice_start = time_points[i]
                    
                    # ì—°ì†ì ì¸ ìŒì„± í™•ì¸ (silence_durationë§Œí¼ ì§€ì†ë˜ëŠ”ì§€)
                    consecutive_voice = 0
                    for j in range(i, min(i + int(silence_duration / hop_size), len(energy_values))):
                        if energy_values[j] > energy_threshold:
                            consecutive_voice += hop_size
                        else:
                            break
                    
                    if consecutive_voice >= silence_duration:
                        return max(0, voice_start - 0.05)  # 50ms ì—¬ìœ  ì¶”ê°€
            
            return 0.0  # ìŒì„±ì„ ì°¾ì§€ ëª»í•œ ê²½ìš°
            
        except Exception as e:
            print(f"âŒ ì˜¤ë””ì˜¤ ìŒì„± ê°ì§€ ì‹¤íŒ¨: {e}")
            return 0.0
    
    def _adjust_word_timestamps(self, words: List[Dict], voice_start: float) -> List[Dict]:
        """Word íƒ€ì„ìŠ¤íƒ¬í”„ë¥¼ ì‹¤ì œ ìŒì„± ì‹œì‘ì  ê¸°ì¤€ìœ¼ë¡œ ì¡°ì •"""
        adjusted_words = []
        
        for word in words:
            if hasattr(word, 'start'):
                # Word ê°ì²´ì¸ ê²½ìš°
                adjusted_word = type(word)(
                    word=word.word,
                    start=max(0, word.start - voice_start),
                    end=max(0, word.end - voice_start)
                )
                adjusted_words.append(adjusted_word)
            elif isinstance(word, dict):
                # Dictì¸ ê²½ìš°
                adjusted_word = word.copy()
                adjusted_word['start'] = max(0, word.get('start', 0) - voice_start)
                adjusted_word['end'] = max(0, word.get('end', 0) - voice_start)
                adjusted_words.append(adjusted_word)
        
        return adjusted_words
    
    def _align_with_uniform_distribution(self, syllables: List[str], 
                                       audio_file: str) -> List[SyllableAlignment]:
        """ê· ë“± ë¶„í¬ ê¸°ë°˜ ìŒì ˆ ì •ë ¬"""
        # ì˜¤ë””ì˜¤ ê¸¸ì´ êµ¬í•˜ê¸°
        try:
            sound = pm.Sound(audio_file)
            duration = sound.duration
        except:
            duration = 3.0  # ê¸°ë³¸ê°’
        
        alignments = []
        syllable_duration = duration / len(syllables) if syllables else 1.0
        
        for i, syllable in enumerate(syllables):
            start_time = i * syllable_duration
            end_time = (i + 1) * syllable_duration
            
            # ìëª¨ ë¶„í•´
            initial, medial, final = self.decompose_syllable(syllable)
            
            alignments.append(SyllableAlignment(
                syllable=syllable,
                start_time=start_time,
                end_time=end_time,
                confidence=0.6,  # ë‚®ì€ ì‹ ë¢°ë„ (íƒ€ì„ìŠ¤íƒ¬í”„ ì—†ìŒ)
                word_context="",
                phonetic_features={
                    'initial': initial,
                    'medial': medial,
                    'final': final
                }
            ))
        
        return alignments
    
    def _is_korean(self, char: str) -> bool:
        """í•œêµ­ì–´ ë¬¸ìì¸ì§€ í™•ì¸"""
        return 0xAC00 <= ord(char) <= 0xD7A3 if len(char) == 1 else False


class AdvancedSTTProcessor:
    """
    ê³ ê¸‰ STT ì²˜ë¦¬ ì‹œìŠ¤í…œ (ê¸°ì¡´ STTProcessor í™•ì¥)
    """
    
    def __init__(self, preferred_engine: str = 'whisper', **engine_configs):
        """
        Parameters:
        -----------
        preferred_engine : str
            ìš°ì„  ì‚¬ìš©í•  STT ì—”ì§„
        engine_configs : dict
            ê° ì—”ì§„ë³„ ì„¤ì • ì •ë³´
        """
        self.stt = UniversalSTT(preferred_engine, **engine_configs)
        self.syllable_aligner = KoreanSyllableAligner()
        
        # ì‹ ë¢°ë„ ì„ê³„ê°’
        self.confidence_threshold = 0.7
        
        print(f"ğŸ¯ ê³ ê¸‰ STT ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ (ì—”ì§„: {self.stt.engine})")
    
    def process_audio_with_confidence(self, audio_file: str, 
                                    target_text: str = "") -> Dict:
        """
        ì‹ ë¢°ë„ í‰ê°€ì™€ í•¨ê»˜ ì˜¤ë””ì˜¤ ì²˜ë¦¬
        """
        print(f"ğŸ¤ ê³ ê¸‰ STT ì²˜ë¦¬ ì‹œì‘: {Path(audio_file).name}")
        
        # STT ì „ì‚¬
        transcription = self.stt.transcribe(audio_file, language='ko', return_timestamps=True)
        
        # ëª©í‘œ í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ ì¼ì¹˜ë„ ê²€ì‚¬
        if target_text:
            similarity = self._calculate_text_similarity(transcription.text, target_text)
            print(f"ğŸ“Š í…ìŠ¤íŠ¸ ì¼ì¹˜ë„: {similarity:.2%}")
            
            # ì¼ì¹˜ë„ê°€ ë‚®ìœ¼ë©´ ëª©í‘œ í…ìŠ¤íŠ¸ ì‚¬ìš©
            if similarity < 0.7:
                print(f"âš ï¸ ì¼ì¹˜ë„ ë‚®ìŒ, ëª©í‘œ í…ìŠ¤íŠ¸ ì‚¬ìš©: {target_text}")
                transcription.text = target_text
                transcription.confidence = 0.8  # ìˆ˜ë™ ì…ë ¥ ì‹ ë¢°ë„
        
        # ìŒì ˆ ì •ë ¬
        syllable_alignments = self.syllable_aligner.align_syllables_with_timestamps(
            transcription, audio_file
        )
        
        # ì‹ ë¢°ë„ í‰ê°€
        overall_confidence = self._evaluate_overall_confidence(transcription, syllable_alignments)
        
        return {
            'transcription': transcription.text,
            'syllables': [
                {
                    'label': sa.syllable,
                    'start': sa.start_time,
                    'end': sa.end_time,
                    'confidence': sa.confidence,
                    'phonetic_features': sa.phonetic_features
                }
                for sa in syllable_alignments
            ],
            'confidence': overall_confidence,
            'engine': transcription.engine,
            'word_timestamps': transcription.words,
            'quality_metrics': {
                'syllable_count': len(syllable_alignments),
                'avg_syllable_confidence': np.mean([sa.confidence for sa in syllable_alignments]),
                'has_word_timestamps': len(transcription.words) > 0
            }
        }
    
    def _calculate_text_similarity(self, text1: str, text2: str) -> float:
        """í•œêµ­ì–´ íŠ¹í™” í…ìŠ¤íŠ¸ ìœ ì‚¬ë„ ê³„ì‚° (ê°œì„ ëœ êµ¬í˜„)"""
        # í•œêµ­ì–´ íŠ¹í™” ì „ì²˜ë¦¬
        clean1 = self._preprocess_korean_text(text1)
        clean2 = self._preprocess_korean_text(text2)
        
        if not clean1 or not clean2:
            return 0.0
        
        # 1. ìŒì ˆ ë‹¨ìœ„ ìœ ì‚¬ë„ (ê°€ì¤‘ì¹˜: 0.6)
        syllable_similarity = self._calculate_syllable_similarity(clean1, clean2)
        
        # 2. ìëª¨ ë‹¨ìœ„ ìœ ì‚¬ë„ (ê°€ì¤‘ì¹˜: 0.3)
        jamo_similarity = self._calculate_jamo_similarity(clean1, clean2)
        
        # 3. ê¸¸ì´ ìœ ì‚¬ë„ (ê°€ì¤‘ì¹˜: 0.1)
        len1, len2 = len(clean1), len(clean2)
        length_similarity = 1.0 - abs(len1 - len2) / max(len1, len2)
        
        # ê°€ì¤‘ í‰ê· 
        overall_similarity = (
            0.6 * syllable_similarity +
            0.3 * jamo_similarity +
            0.1 * length_similarity
        )
        
        return min(1.0, overall_similarity)
    
    def _preprocess_korean_text(self, text: str) -> str:
        """í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
        import re
        
        # íŠ¹ìˆ˜ë¬¸ì, ê³µë°±, êµ¬ë‘ì  ì œê±°
        cleaned = re.sub(r'[^\uAC00-\uD7A3\u1100-\u11FF\u3130-\u318F]', '', text)
        
        # ììŒ, ëª¨ìŒ ë‹¨ë… ì œê±° (ì™„ì„±í˜• í•œê¸€ë§Œ ìœ ì§€)
        korean_syllables = re.findall(r'[\uAC00-\uD7A3]', cleaned)
        
        return ''.join(korean_syllables)
    
    def _calculate_syllable_similarity(self, text1: str, text2: str) -> float:
        """ìŒì ˆ ë‹¨ìœ„ ìœ ì‚¬ë„ ê³„ì‚°"""
        if not text1 or not text2:
            return 0.0
        
        # ë™ì  í”„ë¡œê·¸ë˜ë°ìœ¼ë¡œ ìµœì¥ ê³µí†µ ë¶€ë¶„ ì‹œí€€ìŠ¤ ê³„ì‚°
        len1, len2 = len(text1), len(text2)
        dp = [[0] * (len2 + 1) for _ in range(len1 + 1)]
        
        for i in range(1, len1 + 1):
            for j in range(1, len2 + 1):
                if text1[i-1] == text2[j-1]:
                    dp[i][j] = dp[i-1][j-1] + 1
                else:
                    dp[i][j] = max(dp[i-1][j], dp[i][j-1])
        
        lcs_length = dp[len1][len2]
        return lcs_length / max(len1, len2)
    
    def _calculate_jamo_similarity(self, text1: str, text2: str) -> float:
        """ìëª¨ ë‹¨ìœ„ ìœ ì‚¬ë„ ê³„ì‚°"""
        try:
            # ê° ìŒì ˆì„ ìëª¨ë¡œ ë¶„í•´
            jamo1 = []
            jamo2 = []
            
            for char in text1:
                if 0xAC00 <= ord(char) <= 0xD7A3:  # ì™„ì„±í˜• í•œê¸€
                    initial, medial, final = self._decompose_hangul(char)
                    jamo1.extend([initial, medial, final] if final else [initial, medial])
            
            for char in text2:
                if 0xAC00 <= ord(char) <= 0xD7A3:
                    initial, medial, final = self._decompose_hangul(char)
                    jamo2.extend([initial, medial, final] if final else [initial, medial])
            
            if not jamo1 or not jamo2:
                return 0.0
            
            # ìëª¨ ë§¤ì¹­ ê³„ì‚°
            matching = sum(1 for j1, j2 in zip(jamo1, jamo2) if j1 == j2)
            return matching / max(len(jamo1), len(jamo2))
            
        except Exception:
            return 0.0
    
    def _decompose_hangul(self, char: str) -> tuple:
        """í•œê¸€ ìŒì ˆì„ ìëª¨ë¡œ ë¶„í•´"""
        if len(char) != 1 or not (0xAC00 <= ord(char) <= 0xD7A3):
            return ('', '', '')
        
        # í•œê¸€ ìœ ë‹ˆì½”ë“œ ë¶„í•´
        code = ord(char) - 0xAC00
        
        # ì´ˆì„±, ì¤‘ì„±, ì¢…ì„± ì¸ë±ìŠ¤
        initial_idx = code // (21 * 28)
        medial_idx = (code % (21 * 28)) // 28
        final_idx = code % 28
        
        # ìëª¨ í…Œì´ë¸”
        initials = ['ã„±', 'ã„²', 'ã„´', 'ã„·', 'ã„¸', 'ã„¹', 'ã…', 'ã…‚', 'ã…ƒ', 'ã……', 'ã…†', 'ã…‡', 'ã…ˆ', 'ã…‰', 'ã…Š', 'ã…‹', 'ã…Œ', 'ã…', 'ã…']
        medials = ['ã…', 'ã…', 'ã…‘', 'ã…’', 'ã…“', 'ã…”', 'ã…•', 'ã…–', 'ã…—', 'ã…˜', 'ã…™', 'ã…š', 'ã…›', 'ã…œ', 'ã…', 'ã…', 'ã…Ÿ', 'ã… ', 'ã…¡', 'ã…¢', 'ã…£']
        finals = ['', 'ã„±', 'ã„²', 'ã„³', 'ã„´', 'ã„µ', 'ã„¶', 'ã„·', 'ã„¹', 'ã„º', 'ã„»', 'ã„¼', 'ã„½', 'ã„¾', 'ã„¿', 'ã…€', 'ã…', 'ã…‚', 'ã…„', 'ã……', 'ã…†', 'ã…‡', 'ã…ˆ', 'ã…Š', 'ã…‹', 'ã…Œ', 'ã…', 'ã…']
        
        initial = initials[initial_idx]
        medial = medials[medial_idx]
        final = finals[final_idx]
        
        return (initial, medial, final)
    
    def _evaluate_overall_confidence(self, transcription: TranscriptionResult, 
                                   syllables: List[SyllableAlignment]) -> float:
        """ì „ì²´ ì‹ ë¢°ë„ í‰ê°€"""
        factors = []
        
        # STT ì—”ì§„ë³„ ê¸°ë³¸ ì‹ ë¢°ë„
        engine_confidence = {
            'whisper': 0.85,
            'google': 0.90,
            'azure': 0.88,
            'naver_clova': 0.80,
            'local_fallback': 0.50
        }
        factors.append(engine_confidence.get(transcription.engine, 0.60))
        
        # ì „ì‚¬ ì‹ ë¢°ë„
        factors.append(transcription.confidence)
        
        # ìŒì ˆ ì •ë ¬ ì‹ ë¢°ë„
        if syllables:
            syllable_confidence = np.mean([s.confidence for s in syllables])
            factors.append(syllable_confidence)
        
        # íƒ€ì„ìŠ¤íƒ¬í”„ ì¡´ì¬ ì—¬ë¶€
        if transcription.words:
            factors.append(0.9)  # íƒ€ì„ìŠ¤íƒ¬í”„ ìˆìœ¼ë©´ ë³´ë„ˆìŠ¤
        else:
            factors.append(0.6)
        
        return np.mean(factors)
    
    def get_engine_status(self) -> Dict:
        """STT ì—”ì§„ ìƒíƒœ ì •ë³´"""
        return {
            'current_engine': self.stt.engine,
            'available_engines': self.stt.available_engines,
            'confidence_threshold': self.confidence_threshold
        }


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == "__main__":
    # ê³ ê¸‰ STT í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”
    processor = AdvancedSTTProcessor(
        preferred_engine='whisper',
        model_size='base'
    )
    
    # í…ŒìŠ¤íŠ¸ íŒŒì¼ ì²˜ë¦¬
    test_file = "static/reference_files/ë‚­ë…ë¬¸ì¥.wav"
    if Path(test_file).exists():
        result = processor.process_audio_with_confidence(
            test_file, 
            target_text="í•˜ë‚˜ë„ ë†“ì¹˜ì§€ ì•Šê³  ì—´ì‹¬íˆ ë³´ê³  ìˆìŠµë‹ˆë‹¤"
        )
        print(f"ğŸ¯ ì²˜ë¦¬ ê²°ê³¼: {result}")