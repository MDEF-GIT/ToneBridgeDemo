import React, { useState, useRef, useEffect, useCallback } from "react";
import { YINPitchDetector, PitchResult } from '../utils/pitchAnalysis';
import { AudioPlaybackController } from '../utils/audioUtils';

interface AudioRecordingState {
  isRecording: boolean;
  audioStream: MediaStream | null;
  audioContext: AudioContext | null;
  analyser: AnalyserNode | null;
  error: string | null;
  recordedBlob: Blob | null;
  isPlayingRecorded: boolean;
  // ğŸ¯ ìƒˆë¡œìš´ ê³ ê¸‰ í”¼ì¹˜ ë¶„ì„ ìƒíƒœ
  advancedPitchData: PitchResult[];
  currentPitchConfidence: number;
  // ğŸ¯ ìë™ ì²˜ë¦¬ ê²°ê³¼
  autoProcessResult: any | null;
}

export const useAudioRecording = (learnerInfo?: {name: string, gender: string, ageGroup: string}, selectedFile?: string) => {
  const [state, setState] = useState<AudioRecordingState>({
    isRecording: false,
    audioStream: null,
    audioContext: null,
    analyser: null,
    error: null,
    recordedBlob: null,
    isPlayingRecorded: false,
    // ğŸ¯ ìƒˆë¡œìš´ ê³ ê¸‰ í”¼ì¹˜ ë¶„ì„ ìƒíƒœ
    advancedPitchData: [],
    currentPitchConfidence: 0,
    // ğŸ¯ ìë™ ì²˜ë¦¬ ê²°ê³¼
    autoProcessResult: null,
  });

  const animationFrameRef = useRef<number | undefined>(undefined);
  const onPitchDataRef = useRef<
    ((frequency: number, timestamp: number) => void) | null
  >(null);
  const mediaRecorderRef = useRef<MediaRecorder | null>(null);
  const audioChunksRef = useRef<Blob[]>([]);
  const recordedAudioRef = useRef<HTMLAudioElement | null>(null);
  
  // ğŸ¯ ìµœì‹  ê°’ì„ ì°¸ì¡°í•˜ê¸° ìœ„í•œ refë“¤
  const learnerInfoRef = useRef(learnerInfo);
  const selectedFileRef = useRef(selectedFile);
  
  // ğŸ¯ ìƒˆë¡œìš´ ê³ ê¸‰ í”¼ì¹˜ ë¶„ì„ ì—”ì§„
  const yinDetectorRef = useRef<YINPitchDetector | null>(null);
  const audioPlaybackRef = useRef<AudioPlaybackController>(new AudioPlaybackController());
  
  // ğŸ¯ ref ê°’ ì—…ë°ì´íŠ¸
  useEffect(() => {
    learnerInfoRef.current = learnerInfo;
    selectedFileRef.current = selectedFile;
  }, [learnerInfo, selectedFile]);

  // ğŸ¯ ìƒíƒœ ë³€í™” ì¶”ì  ë¡œê·¸
  useEffect(() => {
    console.log('ğŸ¯ [STEP 3] Hook ìƒíƒœ ë³€í™” ê°ì§€:', {
      'state.isPlayingRecorded': state.isPlayingRecorded,
      'state.recordedBlob': !!state.recordedBlob,
      'recordedAudioRef.current': !!recordedAudioRef.current
    });
  }, [state.isPlayingRecorded, state.recordedBlob]);

  const startRecording = useCallback(async () => {
    try {
      // ğŸš¨ ë…¹ìŒ ì‹œì‘ ì „ í•„ìˆ˜ ì •ë³´ ì²´í¬ (ìµœì‹  ref ê°’ ì‚¬ìš©)
      const currentLearnerInfo = learnerInfoRef.current;
      const currentSelectedFile = selectedFileRef.current;
      
      if (!currentLearnerInfo || !currentLearnerInfo.name || !currentLearnerInfo.gender) {
        alert("âš ï¸ í•™ìŠµì ì •ë³´ë¥¼ ë¨¼ì € ì…ë ¥í•´ì£¼ì„¸ìš”!\n\n- ì´ë¦„ê³¼ ì„±ë³„ì€ í•„ìˆ˜ ì…ë ¥ ì‚¬í•­ì…ë‹ˆë‹¤.");
        return;
      }
      
      if (!currentSelectedFile) {
        alert("âš ï¸ ì—°ìŠµë¬¸ì¥ì„ ë¨¼ì € ì„ íƒí•´ì£¼ì„¸ìš”!\n\n- 10ê°œ ë¬¸ì¥ ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•´ì•¼ ë…¹ìŒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.");
        return;
      }
      
      console.log("âœ… ëª¨ë“  í•„ìˆ˜ ì •ë³´ í™•ì¸ ì™„ë£Œ - ë…¹ìŒ ì‹œì‘");
      console.log("ğŸ“‹ í•™ìŠµì:", `${currentLearnerInfo.name} (${currentLearnerInfo.gender}, ${currentLearnerInfo.ageGroup || 'ì—°ë ¹ ë¯¸ì§€ì •'})`);
      console.log("ğŸ“„ ì—°ìŠµë¬¸ì¥:", currentSelectedFile);
      
      const stream = await navigator.mediaDevices.getUserMedia({
        audio: {
          echoCancellation: false,
          noiseSuppression: false,
          autoGainControl: false,
          sampleRate: 44100,
        },
      });

      const audioContext = new AudioContext({ sampleRate: 44100 });
      const analyser = audioContext.createAnalyser();
      const source = audioContext.createMediaStreamSource(stream);

      analyser.fftSize = 4096;
      analyser.smoothingTimeConstant = 0.3;
      source.connect(analyser);
      
      // ğŸ¯ ê³ ê¸‰ YIN í”¼ì¹˜ ê²€ì¶œê¸° ì´ˆê¸°í™”
      yinDetectorRef.current = new YINPitchDetector(audioContext.sampleRate, {
        frameMs: 25,
        confidenceThreshold: 0.6,
        voicingThreshold: 0.45
      });

      // Setup MediaRecorder for file saving
      const mediaRecorder = new MediaRecorder(stream, {
        mimeType: "audio/webm;codecs=opus",
      });

      audioChunksRef.current = [];

      mediaRecorder.ondataavailable = (event) => {
        if (event.data.size > 0) {
          audioChunksRef.current.push(event.data);
        }
      };

      mediaRecorder.onstop = async () => {
        const audioBlob = new Blob(audioChunksRef.current, {
          type: "audio/webm",
        });

        setState((prev) => ({
          ...prev,
          recordedBlob: audioBlob,
          // ğŸ¯ í”¼ì¹˜ ë¶„ì„ ë°ì´í„° ì´ˆê¸°í™”
          advancedPitchData: [],
          currentPitchConfidence: 0,
        }));

        // Upload to backend
        await uploadRecordedAudio(audioBlob);
      };

      mediaRecorderRef.current = mediaRecorder;
      mediaRecorder.start(1000); // Record in 1-second chunks

      setState((prev) => ({
        ...prev,
        isRecording: true,
        audioStream: stream,
        audioContext,
        analyser,
        error: null,
      }));

      // Start pitch detection
      const detectPitch = () => {
        if (!analyser) return;

        const bufferLength = analyser.frequencyBinCount;
        const dataArray = new Float32Array(bufferLength);
        analyser.getFloatFrequencyData(dataArray);

        // Simple pitch detection using autocorrelation
        const sampleRate = audioContext.sampleRate;
        const timeDomainData = new Float32Array(analyser.fftSize);
        analyser.getFloatTimeDomainData(timeDomainData);

        const frequency = autoCorrelate(timeDomainData, sampleRate);

        if (frequency > 0 && onPitchDataRef.current) {
          onPitchDataRef.current(frequency, Date.now());
        }

        animationFrameRef.current = requestAnimationFrame(detectPitch);
      };

      detectPitch();
    } catch (error) {
      setState((prev) => ({
        ...prev,
        error: `ë§ˆì´í¬ ì ‘ê·¼ ì‹¤íŒ¨: ${error instanceof Error ? error.message : "ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜"}`,
      }));
    }
  }, [learnerInfo, selectedFile]);

  const stopRecording = useCallback(() => {
    if (animationFrameRef.current) {
      cancelAnimationFrame(animationFrameRef.current);
    }

    if (
      mediaRecorderRef.current &&
      mediaRecorderRef.current.state !== "inactive"
    ) {
      mediaRecorderRef.current.stop();
    }

    if (state.audioStream) {
      state.audioStream.getTracks().forEach((track) => track.stop());
    }

    if (state.audioContext) {
      state.audioContext.close();
    }

    setState((prev) => ({
      ...prev,
      isRecording: false,
      audioStream: null,
      audioContext: null,
      analyser: null,
      error: null,
      recordedBlob: null,
    }));
  }, [state, learnerInfo, selectedFile]);

  const uploadRecordedAudio = async (audioBlob: Blob) => {
    try {
      // ğŸ¯ ìµœì‹  ref ê°’ ì‚¬ìš©
      const currentLearnerInfo = learnerInfoRef.current;
      const currentSelectedFile = selectedFileRef.current;
      
      // ğŸš¨ ì‚¬ìš©ìì—ê²Œ ì§ì ‘ ë³´ì—¬ì£¼ëŠ” ë””ë²„ê¹…
      let debugInfo = "ğŸ¤ ë…¹ìŒ ì™„ë£Œ!\n\n";
      debugInfo += `ğŸ“‹ í•™ìŠµì ì •ë³´:\n`;
      debugInfo += `  - ì´ë¦„: ${currentLearnerInfo?.name || "âŒ ì—†ìŒ"}\n`;
      debugInfo += `  - ì„±ë³„: ${currentLearnerInfo?.gender || "âŒ ì—†ìŒ"}\n`;
      debugInfo += `  - ì—°ë ¹ëŒ€: ${currentLearnerInfo?.ageGroup || "âŒ ì—†ìŒ"}\n`;
      debugInfo += `ğŸ“„ ì„ íƒëœ ë¬¸ì¥: ${currentSelectedFile || "âŒ ì—†ìŒ"}\n\n`;
      debugInfo += "ì´ ì •ë³´ë¡œ íŒŒì¼ì„ ì €ì¥í•©ë‹ˆë‹¤.";
      
      alert(debugInfo);
      
      const formData = new FormData();
      formData.append("file", audioBlob, "recording.webm");
      formData.append("sentence_hint", "");
      formData.append("save_permanent", "true");
      
      // í•™ìŠµì ì •ë³´ ì¶”ê°€
      if (currentLearnerInfo) {
        formData.append("learner_name", currentLearnerInfo.name || "");
        formData.append("learner_gender", currentLearnerInfo.gender || "");
        formData.append("learner_age_group", currentLearnerInfo.ageGroup || "");
      }
      
      // ì—°ìŠµë¬¸ì¥ ì¶”ê°€
      if (currentSelectedFile) {
        formData.append("reference_sentence", currentSelectedFile);
      }

      const response = await fetch("/api/auto-process", {
        method: "POST",
        body: formData,
      });

      if (response.ok) {
        const result = await response.json();
        console.log("âœ… ìë™ ì²˜ë¦¬ ì™„ë£Œ:", result);
        
        // ì²˜ë¦¬ ê²°ê³¼ ìƒíƒœì— ì €ì¥
        setState((prev) => ({
          ...prev,
          autoProcessResult: result,
        }));
        
        if (result.success) {
          console.log(`ğŸ¯ STT ê²°ê³¼: "${result.transcription}"`);
          console.log(`ğŸ”¤ ${result.syllables?.length || 0}ê°œ ìŒì ˆ ë¶„ì ˆ ì™„ë£Œ`);
        }
      } else {
        console.error("âŒ ìë™ ì²˜ë¦¬ ì‹¤íŒ¨:", response.statusText);
      }
    } catch (error) {
      console.error("âŒ ìë™ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜:", error);
    }
  };

  const playRecordedAudio = useCallback(() => {
    console.log('ğŸ¯ğŸ¯ğŸ¯ [STEP 2] playRecordedAudio í•¨ìˆ˜ ì§„ì…!');
    
    // í˜„ì¬ ìƒíƒœ ìƒì„¸ ë¡œê¹…
    console.log('ğŸ¯ [STEP 2.1] í˜„ì¬ Hook ìƒíƒœ ì²´í¬:', {
      'state.recordedBlob': !!state.recordedBlob,
      'state.isPlayingRecorded': state.isPlayingRecorded,
      'state.isRecording': state.isRecording,
      'audioPlaybackController': !!audioPlaybackRef.current
    });
    
    // í˜„ì¬ ìƒíƒœ í™•ì¸
    if (!state.recordedBlob) {
      console.log("âŒ [STEP 2.2] ë…¹ìŒëœ ìŒì„±ì´ ì—†ìŠµë‹ˆë‹¤ - í•¨ìˆ˜ ì¢…ë£Œ");
      return;
    }
    console.log("âœ… [STEP 2.2] ë…¹ìŒëœ ìŒì„± ì¡´ì¬ í™•ì¸ë¨");

    // í˜„ì¬ ì¬ìƒ ì¤‘ì´ë©´ ì •ì§€ (ìƒˆë¡œìš´ AudioPlaybackController ì‚¬ìš©)
    if (state.isPlayingRecorded) {
      console.log("ğŸ›‘ [STEP 2.3] í˜„ì¬ ì¬ìƒ ì¤‘ - ìƒˆë¡œìš´ ì»¨íŠ¸ë¡¤ëŸ¬ë¡œ ì¤‘ì§€");
      
      audioPlaybackRef.current.stop();
      
      console.log("ğŸ›‘ [STEP 2.3.4] setStateë¡œ isPlayingRecorded: false ì„¤ì •");
      setState(prev => {
        console.log("ğŸ›‘ [STEP 2.3.5] setState ì½œë°± ì‹¤í–‰ - ì´ì „ ìƒíƒœ:", prev.isPlayingRecorded);
        return { ...prev, isPlayingRecorded: false };
      });
      
      console.log("ğŸ›‘ [STEP 2.3.6] ì¤‘ì§€ í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ - í•¨ìˆ˜ ì¢…ë£Œ");
      return;
    }
    console.log("âœ… [STEP 2.3] í˜„ì¬ ì¬ìƒ ì¤‘ì´ ì•„ë‹˜ - ì¬ìƒ ì‹œì‘ í”„ë¡œì„¸ìŠ¤ë¡œ ì§„í–‰");

    // ìƒˆë¡œìš´ AudioPlaybackControllerë¡œ ì¬ìƒ ì‹œì‘
    console.log("â–¶ï¸ [STEP 2.4] ìƒˆë¡œìš´ ì˜¤ë””ì˜¤ ì»¨íŠ¸ë¡¤ëŸ¬ë¡œ ì¬ìƒ ì‹œì‘");
    try {
      audioPlaybackRef.current.play(state.recordedBlob, () => {
        console.log("ğŸ”š [EVENT] ìƒˆë¡œìš´ ì»¨íŠ¸ë¡¤ëŸ¬ - ì¬ìƒ ì™„ë£Œ ì´ë²¤íŠ¸");
        setState(prev => {
          console.log("ğŸ”š [EVENT] setStateë¡œ isPlayingRecorded: false ì„¤ì •");
          return { ...prev, isPlayingRecorded: false };
        });
      }).then(() => {
        console.log("âœ… [STEP 2.4.9] ìƒˆë¡œìš´ ì»¨íŠ¸ë¡¤ëŸ¬ ì¬ìƒ ì„±ê³µ");
        setState(prev => {
          console.log("âœ… [STEP 2.4.10] setStateë¡œ isPlayingRecorded: true ì„¤ì •");
          return { ...prev, isPlayingRecorded: true };
        });
        console.log("âœ… [STEP 2.4.11] ì¬ìƒ ì‹œì‘ í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ");
      }).catch((error) => {
        console.error("âŒ [STEP 2.4.9] ìƒˆë¡œìš´ ì»¨íŠ¸ë¡¤ëŸ¬ ì¬ìƒ ì‹¤íŒ¨:", error);
        setState(prev => {
          console.log("âŒ [STEP 2.4.10] setStateë¡œ isPlayingRecorded: false ì„¤ì •");
          return { ...prev, isPlayingRecorded: false };
        });
        console.log("âŒ [STEP 2.4.11] ì¬ìƒ ì‹¤íŒ¨ ì •ë¦¬ ì‘ì—… ì™„ë£Œ");
      });

    } catch (error) {
      console.error("âŒ [STEP 2.4] try-catch ë¸”ë¡ì—ì„œ ì˜ˆì™¸ ë°œìƒ:", error);
      setState(prev => {
        console.log("âŒ [STEP 2.4.ERROR] setStateë¡œ isPlayingRecorded: false ì„¤ì •");
        return { ...prev, isPlayingRecorded: false };
      });
    }
    
    console.log('ğŸ¯ğŸ¯ğŸ¯ [STEP 2] playRecordedAudio í•¨ìˆ˜ ì¢…ë£Œ');
  }, [state.recordedBlob, state.isPlayingRecorded]);

  const setPitchCallback = useCallback(
    (callback: (frequency: number, timestamp: number) => void) => {
      onPitchDataRef.current = callback;
    },
    [],
  );

  return {
    ...state,
    startRecording,
    stopRecording,
    playRecordedAudio,
    setPitchCallback,
  };
};

// Autocorrelation pitch detection algorithm
function autoCorrelate(buffer: Float32Array, sampleRate: number): number {
  const SIZE = buffer.length;
  const MAX_SAMPLES = Math.floor(SIZE / 2);
  let bestOffset = -1;
  let bestCorrelation = 0;
  let rms = 0;
  let foundGoodCorrelation = false;
  const correlations = new Array(MAX_SAMPLES);

  for (let i = 0; i < SIZE; i++) {
    const val = buffer[i];
    rms += val * val;
  }
  rms = Math.sqrt(rms / SIZE);

  if (rms < 0.01) return -1;

  let lastCorrelation = 1;
  for (let offset = 1; offset < MAX_SAMPLES; offset++) {
    let correlation = 0;
    for (let i = 0; i < MAX_SAMPLES; i++) {
      correlation += Math.abs(buffer[i] - buffer[i + offset]);
    }
    correlation = 1 - correlation / MAX_SAMPLES;
    correlations[offset] = correlation;

    if (correlation > 0.9 && correlation > lastCorrelation) {
      foundGoodCorrelation = true;
      if (correlation > bestCorrelation) {
        bestCorrelation = correlation;
        bestOffset = offset;
      }
    } else if (foundGoodCorrelation) {
      const shift =
        (correlations[bestOffset + 1] - correlations[bestOffset - 1]) /
        correlations[bestOffset];
      return sampleRate / (bestOffset + 8 * shift);
    }
    lastCorrelation = correlation;
  }

  if (bestCorrelation > 0.01) {
    return sampleRate / bestOffset;
  }
  return -1;
}
