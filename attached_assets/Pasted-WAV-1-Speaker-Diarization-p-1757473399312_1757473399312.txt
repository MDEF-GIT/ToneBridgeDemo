WAV 파일에서 화자 분리 및 메인 화자 추출을 위한 전처리 방법을 단계별로 설명드리겠습니다.
1. 화자 분리 (Speaker Diarization)
주요 도구 및 라이브러리

pyannote.audio: 가장 강력한 화자 분리 도구
SpeechBrain: 딥러닝 기반 음성 처리
resemblyzer: 화자 임베딩 추출
WhisperX: 음성 인식과 화자 분리 동시 수행

기본 구현 예시 (pyannote.audio)
pythonfrom pyannote.audio import Pipeline
import torch

# 화자 분리 파이프라인 로드
pipeline = Pipeline.from_pretrained(
    "pyannote/speaker-diarization-3.1",
    use_auth_token="YOUR_HF_TOKEN"
)

# GPU 사용 설정
pipeline.to(torch.device("cuda"))

# 화자 분리 수행
diarization = pipeline("audio.wav")

# 결과를 RTTM 포맷으로 저장
with open("diarization.rttm", "w") as f:
    diarization.write_rttm(f)
2. 메인 화자 식별 방법
발화 시간 기반 식별
pythonfrom collections import defaultdict

def find_main_speaker(diarization):
    speaker_duration = defaultdict(float)
    
    # 각 화자별 총 발화 시간 계산
    for turn, _, speaker in diarization.itertracks(yield_label=True):
        speaker_duration[speaker] += turn.duration
    
    # 가장 많이 말한 화자 찾기
    main_speaker = max(speaker_duration, key=speaker_duration.get)
    return main_speaker, speaker_duration
화자 임베딩 기반 클러스터링
pythonfrom resemblyzer import VoiceEncoder, preprocess_wav
from scipy.cluster.hierarchy import linkage, fcluster
import numpy as np

encoder = VoiceEncoder()

def extract_speaker_embeddings(audio_segments):
    embeddings = []
    for segment in audio_segments:
        wav = preprocess_wav(segment)
        embedding = encoder.embed_utterance(wav)
        embeddings.append(embedding)
    return np.array(embeddings)
3. 노이즈 및 타 화자 제거
VAD (Voice Activity Detection) 적용
pythonimport webrtcvad
import numpy as np
from pydub import AudioSegment

def apply_vad(audio_path, aggressiveness=3):
    vad = webrtcvad.Vad(aggressiveness)  # 0-3, 높을수록 엄격
    
    audio = AudioSegment.from_wav(audio_path)
    # 16kHz, 16-bit로 변환
    audio = audio.set_frame_rate(16000).set_sample_width(2)
    
    frame_duration = 30  # ms
    frames = []
    
    for i in range(0, len(audio), frame_duration):
        frame = audio[i:i+frame_duration]
        if len(frame) == frame_duration:
            is_speech = vad.is_speech(frame.raw_data, 16000)
            if is_speech:
                frames.append(frame)
    
    return sum(frames)
스펙트럼 게이팅을 통한 노이즈 제거
pythonimport noisereduce as nr
import librosa

def reduce_noise(audio_path):
    # 오디오 로드
    y, sr = librosa.load(audio_path, sr=None)
    
    # 노이즈 감소
    reduced_noise = nr.reduce_noise(
        y=y, 
        sr=sr,
        stationary=True,  # 정상 노이즈
        prop_decrease=1.0  # 노이즈 감소 강도
    )
    
    return reduced_noise, sr
4. 통합 파이프라인
pythondef preprocess_audio_pipeline(audio_path):
    # 1. 노이즈 제거
    cleaned_audio, sr = reduce_noise(audio_path)
    
    # 2. VAD 적용
    speech_only = apply_vad(audio_path)
    
    # 3. 화자 분리
    diarization = pipeline(audio_path)
    
    # 4. 메인 화자 식별
    main_speaker, durations = find_main_speaker(diarization)
    
    # 5. 메인 화자 세그먼트만 추출
    main_segments = []
    for segment, _, speaker in diarization.itertracks(yield_label=True):
        if speaker == main_speaker:
            start = int(segment.start * sr)
            end = int(segment.end * sr)
            main_segments.append(cleaned_audio[start:end])
    
    # 6. 세그먼트 연결
    final_audio = np.concatenate(main_segments)
    
    return final_audio, sr
5. 발화 경계 정제
크로스페이드 적용
pythondef apply_crossfade(segments, fade_duration=0.05, sr=16000):
    fade_samples = int(fade_duration * sr)
    merged = []
    
    for i, segment in enumerate(segments):
        if i > 0:
            # 이전 세그먼트와 크로스페이드
            prev_end = merged[-fade_samples:]
            curr_start = segment[:fade_samples]
            
            # 선형 크로스페이드
            fade_out = np.linspace(1, 0, fade_samples)
            fade_in = np.linspace(0, 1, fade_samples)
            
            crossfade = prev_end * fade_out + curr_start * fade_in
            merged[-fade_samples:] = crossfade
            merged.extend(segment[fade_samples:])
        else:
            merged.extend(segment)
    
    return np.array(merged)
6. 고급 기법
Separation by Diarization (화자별 음원 분리)
pythonfrom pyannote.audio import Model
from pyannote.audio.pipelines import SpeakerDiarization

# 화자별 마스크 생성
def create_speaker_masks(diarization, audio_length, sr):
    masks = {}
    for segment, _, speaker in diarization.itertracks(yield_label=True):
        if speaker not in masks:
            masks[speaker] = np.zeros(audio_length)
        
        start = int(segment.start * sr)
        end = int(segment.end * sr)
        masks[speaker][start:end] = 1
    
    return masks
실시간 처리를 위한 스트리밍 방식
pythondef streaming_vad(audio_stream, chunk_size=480):
    vad = webrtcvad.Vad(2)
    
    for chunk in audio_stream:
        if vad.is_speech(chunk, sample_rate=16000):
            yield chunk
추천 워크플로우

전처리: 노이즈 제거 → VAD → 정규화
화자 분리: pyannote.audio 또는 WhisperX 사용
메인 화자 선택: 발화 시간 또는 첫 화자 기준
세그먼트 추출: 메인 화자 구간만 선택
후처리: 크로스페이드, 무음 제거

이러한 방법들을 조합하여 사용하면 효과적으로 메인 화자의 음성만 추출할 수 있습니다. 특히 pyannote.audio 3.1 버전은 매우 높은 정확도를 보이므로 추천드립니다.