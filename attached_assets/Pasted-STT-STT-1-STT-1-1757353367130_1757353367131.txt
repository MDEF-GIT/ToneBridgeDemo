STT 기반 자동 음절 분절 시스템
목차

STT 엔진 통합
텍스트-음성 정렬 시스템
한국어 특화 음절 분절
다국어 지원 시스템
완전 통합 파이프라인


1. STT 엔진 통합
1.1 다중 STT 엔진 지원
pythonimport whisper
import torch
from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC
from google.cloud import speech_v1
import azure.cognitiveservices.speech as speechsdk
import librosa
import numpy as np
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass
import json

@dataclass
class TranscriptionResult:
    """전사 결과 클래스"""
    text: str
    language: str
    confidence: float
    words: List[Dict]  # [{'word': str, 'start': float, 'end': float, 'confidence': float}]
    segments: List[Dict]  # 문장/구 단위 세그먼트
    
class UniversalSTT:
    """
    다중 STT 엔진 통합 클래스
    """
    
    def __init__(self, engine: str = 'whisper', **kwargs):
        """
        Parameters:
        -----------
        engine : str
            STT 엔진 선택 ('whisper', 'wav2vec2', 'google', 'azure', 'naver_clova')
        """
        self.engine = engine
        self.model = None
        self.processor = None
        
        self._initialize_engine(**kwargs)
    
    def _initialize_engine(self, **kwargs):
        """엔진 초기화"""
        if self.engine == 'whisper':
            self._init_whisper(**kwargs)
        elif self.engine == 'wav2vec2':
            self._init_wav2vec2(**kwargs)
        elif self.engine == 'google':
            self._init_google(**kwargs)
        elif self.engine == 'azure':
            self._init_azure(**kwargs)
        elif self.engine == 'naver_clova':
            self._init_naver_clova(**kwargs)
        else:
            raise ValueError(f"Unsupported engine: {self.engine}")
    
    def _init_whisper(self, model_size: str = 'large', device: str = None):
        """OpenAI Whisper 초기화"""
        if device is None:
            device = 'cuda' if torch.cuda.is_available() else 'cpu'
        
        self.model = whisper.load_model(model_size, device=device)
        print(f"Whisper {model_size} model loaded on {device}")
    
    def _init_wav2vec2(self, model_name: str = 'facebook/wav2vec2-large-960h-lv60-self'):
        """Wav2Vec2 초기화"""
        self.processor = Wav2Vec2Processor.from_pretrained(model_name)
        self.model = Wav2Vec2ForCTC.from_pretrained(model_name)
        
        if torch.cuda.is_available():
            self.model = self.model.cuda()
    
    def _init_google(self, credentials_path: str = None):
        """Google Cloud Speech-to-Text 초기화"""
        if credentials_path:
            import os
            os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = credentials_path
        
        self.client = speech_v1.SpeechClient()
    
    def _init_azure(self, subscription_key: str, region: str):
        """Azure Speech Services 초기화"""
        speech_config = speechsdk.SpeechConfig(
            subscription=subscription_key,
            region=region
        )
        self.speech_config = speech_config
    
    def _init_naver_clova(self, client_id: str, client_secret: str):
        """Naver CLOVA Speech 초기화"""
        self.clova_config = {
            'client_id': client_id,
            'client_secret': client_secret,
            'url': 'https://naveropenapi.apigw.ntruss.com/recog/v1/stt'
        }
    
    def transcribe(self, audio_file: str, language: str = None, 
                  return_timestamps: bool = True) -> TranscriptionResult:
        """
        음성 파일 전사
        
        Parameters:
        -----------
        audio_file : str
            입력 오디오 파일 경로
        language : str
            언어 코드 (예: 'ko', 'en', 'ja', 'zh')
        return_timestamps : bool
            타임스탬프 반환 여부
        
        Returns:
        --------
        TranscriptionResult : 전사 결과
        """
        if self.engine == 'whisper':
            return self._transcribe_whisper(audio_file, language, return_timestamps)
        elif self.engine == 'wav2vec2':
            return self._transcribe_wav2vec2(audio_file, language)
        elif self.engine == 'google':
            return self._transcribe_google(audio_file, language, return_timestamps)
        elif self.engine == 'azure':
            return self._transcribe_azure(audio_file, language, return_timestamps)
        elif self.engine == 'naver_clova':
            return self._transcribe_naver_clova(audio_file, language)
        else:
            raise ValueError(f"Engine {self.engine} not supported")
    
    def _transcribe_whisper(self, audio_file: str, language: str = None,
                           return_timestamps: bool = True) -> TranscriptionResult:
        """Whisper로 전사"""
        # 옵션 설정
        options = {
            'word_timestamps': return_timestamps,
            'verbose': False
        }
        
        if language:
            options['language'] = language
        
        # 전사 실행
        result = self.model.transcribe(audio_file, **options)
        
        # 결과 파싱
        words = []
        if return_timestamps and 'words' in result:
            for segment in result['segments']:
                if 'words' in segment:
                    for word_info in segment['words']:
                        words.append({
                            'word': word_info['word'].strip(),
                            'start': word_info['start'],
                            'end': word_info['end'],
                            'confidence': word_info.get('probability', 0.0)
                        })
        
        # 세그먼트 정보
        segments = []
        for segment in result['segments']:
            segments.append({
                'id': segment['id'],
                'text': segment['text'].strip(),
                'start': segment['start'],
                'end': segment['end'],
                'confidence': segment.get('avg_logprob', 0.0)
            })
        
        return TranscriptionResult(
            text=result['text'].strip(),
            language=result.get('language', language or 'unknown'),
            confidence=np.mean([s['confidence'] for s in segments]) if segments else 0.0,
            words=words,
            segments=segments
        )
    
    def _transcribe_wav2vec2(self, audio_file: str, language: str = None) -> TranscriptionResult:
        """Wav2Vec2로 전사"""
        # 오디오 로드
        audio, sr = librosa.load(audio_file, sr=16000)
        
        # 토크나이징
        inputs = self.processor(audio, sampling_rate=sr, return_tensors="pt", padding=True)
        
        if torch.cuda.is_available():
            inputs = {k: v.cuda() for k, v in inputs.items()}
        
        # 추론
        with torch.no_grad():
            logits = self.model(inputs.input_values).logits
        
        # 디코딩
        predicted_ids = torch.argmax(logits, dim=-1)
        transcription = self.processor.batch_decode(predicted_ids)[0]
        
        # CTC 정렬로 단어 타임스탬프 추출
        words = self._extract_word_timestamps_ctc(
            logits, 
            self.processor, 
            audio_file
        )
        
        return TranscriptionResult(
            text=transcription,
            language=language or 'en',
            confidence=0.0,  # Wav2Vec2는 confidence 제공 안함
            words=words,
            segments=[{'text': transcription, 'start': 0, 'end': len(audio)/sr}]
        )
    
    def _transcribe_google(self, audio_file: str, language: str = 'ko-KR',
                         return_timestamps: bool = True) -> TranscriptionResult:
        """Google Cloud Speech-to-Text로 전사"""
        # 오디오 파일 읽기
        with open(audio_file, 'rb') as f:
            content = f.read()
        
        # 오디오 설정
        audio = speech_v1.RecognitionAudio(content=content)
        
        # 설정
        config = speech_v1.RecognitionConfig(
            encoding=speech_v1.RecognitionConfig.AudioEncoding.LINEAR16,
            sample_rate_hertz=16000,
            language_code=language,
            enable_word_time_offsets=return_timestamps,
            enable_automatic_punctuation=True,
            model='latest_long'
        )
        
        # 전사 실행
        response = self.client.recognize(config=config, audio=audio)
        
        # 결과 파싱
        text = ""
        words = []
        segments = []
        
        for result in response.results:
            text += result.alternatives[0].transcript + " "
            
            segments.append({
                'text': result.alternatives[0].transcript,
                'confidence': result.alternatives[0].confidence
            })
            
            if return_timestamps:
                for word_info in result.alternatives[0].words:
                    words.append({
                        'word': word_info.word,
                        'start': word_info.start_time.total_seconds(),
                        'end': word_info.end_time.total_seconds(),
                        'confidence': result.alternatives[0].confidence
                    })
        
        return TranscriptionResult(
            text=text.strip(),
            language=language,
            confidence=np.mean([s['confidence'] for s in segments]) if segments else 0.0,
            words=words,
            segments=segments
        )
    
    def _transcribe_naver_clova(self, audio_file: str, language: str = 'ko-KR') -> TranscriptionResult:
        """Naver CLOVA Speech로 전사"""
        import requests
        
        # 오디오 파일 읽기
        with open(audio_file, 'rb') as f:
            data = f.read()
        
        # 헤더 설정
        headers = {
            'X-NCP-APIGW-API-KEY-ID': self.clova_config['client_id'],
            'X-NCP-APIGW-API-KEY': self.clova_config['client_secret'],
            'Content-Type': 'application/octet-stream'
        }
        
        # 파라미터
        params = {
            'lang': language.split('-')[0]  # 'ko-KR' -> 'ko'
        }
        
        # API 호출
        response = requests.post(
            self.clova_config['url'],
            headers=headers,
            params=params,
            data=data
        )
        
        if response.status_code == 200:
            result = response.json()
            return TranscriptionResult(
                text=result.get('text', ''),
                language=language,
                confidence=1.0,  # CLOVA는 confidence 제공 안함
                words=[],  # CLOVA는 단어 타임스탬프 제공 안함
                segments=[{'text': result.get('text', '')}]
            )
        else:
            raise Exception(f"CLOVA API Error: {response.status_code}")
    
    def _extract_word_timestamps_ctc(self, logits, processor, audio_file):
        """CTC 모델에서 단어 타임스탬프 추출"""
        # 간단한 구현 (실제로는 더 복잡한 CTC 정렬 필요)
        words = []
        
        # 토큰 디코딩
        predicted_ids = torch.argmax(logits, dim=-1)
        tokens = processor.batch_decode(predicted_ids, output_word_offsets=True)
        
        # 타임스탬프 계산 (근사)
        if hasattr(tokens, 'word_offsets'):
            for word_info in tokens.word_offsets:
                words.append({
                    'word': word_info['word'],
                    'start': word_info['start_offset'] * 0.02,  # 프레임을 시간으로
                    'end': word_info['end_offset'] * 0.02,
                    'confidence': 0.0
                })
        
        return words
1.2 음절 단위 정렬
pythonclass SyllableAligner:
    """
    STT 결과를 음절 단위로 정렬
    """
    
    def __init__(self):
        self.syllable_splitter = SyllableSplitter()
        
    def align_syllables(self, transcription: TranscriptionResult, 
                       audio_file: str) -> List[Syllable]:
        """
        전사 결과를 음절 단위로 정렬
        
        Parameters:
        -----------
        transcription : TranscriptionResult
            STT 전사 결과
        audio_file : str
            오디오 파일 경로
        
        Returns:
        --------
        List[Syllable] : 정렬된 음절 리스트
        """
        # 언어별 처리
        if transcription.language.startswith('ko'):
            return self._align_korean_syllables(transcription, audio_file)
        elif transcription.language.startswith('en'):
            return self._align_english_syllables(transcription, audio_file)
        elif transcription.language.startswith('ja'):
            return self._align_japanese_syllables(transcription, audio_file)
        elif transcription.language.startswith('zh'):
            return self._align_chinese_syllables(transcription, audio_file)
        else:
            return self._align_generic_syllables(transcription, audio_file)
    
    def _align_korean_syllables(self, transcription: TranscriptionResult,
                               audio_file: str) -> List[Syllable]:
        """한국어 음절 정렬"""
        syllables = []
        
        # 단어 타임스탬프가 있는 경우
        if transcription.words:
            for word_info in transcription.words:
                word = word_info['word']
                start_time = word_info['start']
                end_time = word_info['end']
                
                # 음절 분리
                word_syllables = self.syllable_splitter.split_korean(word)
                
                if word_syllables:
                    # 균등 분할 (더 정교한 방법도 가능)
                    syllable_duration = (end_time - start_time) / len(word_syllables)
                    
                    for i, syl_text in enumerate(word_syllables):
                        syllable = Syllable(
                            start_time=start_time + i * syllable_duration,
                            end_time=start_time + (i + 1) * syllable_duration,
                            text=syl_text,
                            confidence=word_info.get('confidence', 0.0)
                        )
                        syllables.append(syllable)
        
        else:
            # 단어 타임스탬프가 없는 경우 - 음향 특징 기반 분절
            syllables = self._fallback_acoustic_segmentation(
                transcription.text, 
                audio_file, 
                'ko'
            )
        
        return syllables
    
    def _align_english_syllables(self, transcription: TranscriptionResult,
                                audio_file: str) -> List[Syllable]:
        """영어 음절 정렬"""
        import nltk
        from nltk.corpus import cmudict
        
        # CMU 발음 사전 로드
        try:
            d = cmudict.dict()
        except LookupError:
            nltk.download('cmudict')
            d = cmudict.dict()
        
        syllables = []
        
        for word_info in transcription.words:
            word = word_info['word'].lower()
            start_time = word_info['start']
            end_time = word_info['end']
            
            # 음절 수 확인
            if word in d:
                # 발음 사전에서 음절 수 계산
                pronunciation = d[word][0]
                syllable_count = len([p for p in pronunciation if p[-1].isdigit()])
            else:
                # 휴리스틱 음절 계산
                syllable_count = self._count_english_syllables(word)
            
            # 음절 분할
            if syllable_count > 0:
                syllable_duration = (end_time - start_time) / syllable_count
                
                for i in range(syllable_count):
                    syllable = Syllable(
                        start_time=start_time + i * syllable_duration,
                        end_time=start_time + (i + 1) * syllable_duration,
                        text=f"{word}_{i}",  # 음절 텍스트는 근사
                        confidence=word_info.get('confidence', 0.0)
                    )
                    syllables.append(syllable)
        
        return syllables
    
    def _align_japanese_syllables(self, transcription: TranscriptionResult,
                                 audio_file: str) -> List[Syllable]:
        """일본어 음절(모라) 정렬"""
        import MeCab
        
        # MeCab 초기화
        mecab = MeCab.Tagger()
        
        syllables = []
        
        for word_info in transcription.words:
            word = word_info['word']
            start_time = word_info['start']
            end_time = word_info['end']
            
            # 모라 분리
            moras = self.syllable_splitter.split_japanese_mora(word)
            
            if moras:
                mora_duration = (end_time - start_time) / len(moras)
                
                for i, mora in enumerate(moras):
                    syllable = Syllable(
                        start_time=start_time + i * mora_duration,
                        end_time=start_time + (i + 1) * mora_duration,
                        text=mora,
                        confidence=word_info.get('confidence', 0.0)
                    )
                    syllables.append(syllable)
        
        return syllables
    
    def _align_chinese_syllables(self, transcription: TranscriptionResult,
                                audio_file: str) -> List[Syllable]:
        """중국어 음절 정렬"""
        from pypinyin import pinyin, Style
        
        syllables = []
        
        for word_info in transcription.words:
            word = word_info['word']
            start_time = word_info['start']
            end_time = word_info['end']
            
            # 병음으로 변환 (각 한자 = 1음절)
            pinyin_list = pinyin(word, style=Style.NORMAL)
            
            if pinyin_list:
                syllable_duration = (end_time - start_time) / len(pinyin_list)
                
                for i, (char, py) in enumerate(zip(word, pinyin_list)):
                    syllable = Syllable(
                        start_time=start_time + i * syllable_duration,
                        end_time=start_time + (i + 1) * syllable_duration,
                        text=char,
                        confidence=word_info.get('confidence', 0.0),
                        phonemes=[py[0]]  # 병음 저장
                    )
                    syllables.append(syllable)
        
        return syllables
    
    def _fallback_acoustic_segmentation(self, text: str, audio_file: str,
                                       language: str) -> List[Syllable]:
        """
        음향 특징 기반 폴백 분절
        타임스탬프가 없을 때 사용
        """
        # 텍스트를 음절로 분리
        if language == 'ko':
            text_syllables = self.syllable_splitter.split_korean(text)
        else:
            text_syllables = text.split()  # 단어 단위 폴백
        
        # 음향 기반 경계 검출
        segmenter = SyllableSegmentation()
        acoustic_segments = segmenter.segment_syllables(audio_file, method='hybrid')
        
        # 텍스트와 음향 세그먼트 매칭
        aligned_syllables = self._match_text_to_segments(
            text_syllables, 
            acoustic_segments
        )
        
        return aligned_syllables
    
    def _match_text_to_segments(self, text_syllables: List[str],
                               acoustic_segments: List[Syllable]) -> List[Syllable]:
        """텍스트 음절과 음향 세그먼트 매칭"""
        # 개수가 일치하는 경우
        if len(text_syllables) == len(acoustic_segments):
            for i, (text, segment) in enumerate(zip(text_syllables, acoustic_segments)):
                segment.text = text
            return acoustic_segments
        
        # 개수가 다른 경우 - 동적 프로그래밍으로 최적 매칭
        aligned = []
        
        # DTW (Dynamic Time Warping) 방식으로 정렬
        n_text = len(text_syllables)
        n_acoustic = len(acoustic_segments)
        
        if n_text > 0 and n_acoustic > 0:
            # 비용 행렬
            cost = np.zeros((n_text + 1, n_acoustic + 1))
            cost[0, :] = np.inf
            cost[:, 0] = np.inf
            cost[0, 0] = 0
            
            # DP
            for i in range(1, n_text + 1):
                for j in range(1, n_acoustic + 1):
                    # 매칭 비용 (간단히 균등 분포 가정)
                    match_cost = abs(i/n_text - j/n_acoustic)
                    
                    cost[i, j] = match_cost + min(
                        cost[i-1, j],      # 텍스트 스킵
                        cost[i, j-1],      # 음향 스킵
                        cost[i-1, j-1]     # 매칭
                    )
            
            # 백트래킹
            i, j = n_text, n_acoustic
            path = []
            
            while i > 0 and j > 0:
                if cost[i-1, j-1] <= cost[i-1, j] and cost[i-1, j-1] <= cost[i, j-1]:
                    path.append((i-1, j-1))
                    i -= 1
                    j -= 1
                elif cost[i-1, j] < cost[i, j-1]:
                    i -= 1
                else:
                    j -= 1
            
            path.reverse()
            
            # 정렬된 음절 생성
            for text_idx, acoustic_idx in path:
                syllable = acoustic_segments[acoustic_idx]
                syllable.text = text_syllables[text_idx]
                aligned.append(syllable)
        
        return aligned
    
    def _count_english_syllables(self, word: str) -> int:
        """영어 단어의 음절 수 추정 (휴리스틱)"""
        word = word.lower()
        vowels = "aeiouy"
        count = 0
        previous_was_vowel = False
        
        for char in word:
            is_vowel = char in vowels
            if is_vowel and not previous_was_vowel:
                count += 1
            previous_was_vowel = is_vowel
        
        # 끝의 'e'는 보통 무음
        if word.endswith('e'):
            count -= 1
        
        # 최소 1음절
        return max(1, count)

2. 텍스트-음성 정렬 시스템
2.1 강제 정렬 (Forced Alignment)
pythonimport torch
import torchaudio
from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor
import numpy as np

class ForcedAlignment:
    """
    음성과 텍스트의 강제 정렬
    """
    
    def __init__(self, model_name: str = "facebook/wav2vec2-large-960h-lv60-self"):
        self.processor = Wav2Vec2Processor.from_pretrained(model_name)
        self.model = Wav2Vec2ForCTC.from_pretrained(model_name)
        
        if torch.cuda.is_available():
            self.model = self.model.cuda()
    
    def align(self, audio_file: str, transcript: str, 
             language: str = 'en') -> List[Dict]:
        """
        강제 정렬 실행
        
        Parameters:
        -----------
        audio_file : str
            오디오 파일 경로
        transcript : str
            전사 텍스트
        language : str
            언어 코드
        
        Returns:
        --------
        List[Dict] : 정렬된 단어/음절 정보
        """
        # 오디오 로드
        waveform, sample_rate = torchaudio.load(audio_file)
        
        # 리샘플링
        if sample_rate != 16000:
            resampler = torchaudio.transforms.Resample(sample_rate, 16000)
            waveform = resampler(waveform)
        
        # 모델 입력 준비
        input_values = self.processor(
            waveform.squeeze().numpy(), 
            sampling_rate=16000, 
            return_tensors="pt"
        ).input_values
        
        if torch.cuda.is_available():
            input_values = input_values.cuda()
        
        # CTC 출력 얻기
        with torch.no_grad():
            logits = self.model(input_values).logits
        
        # 정렬 수행
        alignment = self._ctc_forced_alignment(
            logits, 
            transcript, 
            self.processor
        )
        
        return alignment
    
    def _ctc_forced_alignment(self, logits, transcript, processor):
        """CTC 기반 강제 정렬"""
        # Softmax 적용
        probs = torch.nn.functional.softmax(logits, dim=-1)
        
        # 토큰화
        tokens = processor.tokenizer(transcript, return_tensors="pt")
        
        # Viterbi 알고리즘으로 최적 경로 찾기
        alignment_path = self._viterbi_alignment(
            probs[0].cpu().numpy(), 
            tokens.input_ids[0].numpy()
        )
        
        # 타임스탬프 계산
        frame_duration = 0.02  # 20ms per frame
        
        aligned_tokens = []
        current_token = None
        start_frame = 0
        
        for frame_idx, token_idx in enumerate(alignment_path):
            if token_idx != current_token:
                if current_token is not None and current_token != 0:  # 0은 blank token
                    aligned_tokens.append({
                        'token': processor.tokenizer.decode([current_token]),
                        'start': start_frame * frame_duration,
                        'end': frame_idx * frame_duration
                    })
                current_token = token_idx
                start_frame = frame_idx
        
        # 마지막 토큰 추가
        if current_token is not None and current_token != 0:
            aligned_tokens.append({
                'token': processor.tokenizer.decode([current_token]),
                'start': start_frame * frame_duration,
                'end': len(alignment_path) * frame_duration
            })
        
        # 토큰을 단어로 그룹화
        words = self._group_tokens_to_words(aligned_tokens)
        
        return words
    
    def _viterbi_alignment(self, probs, tokens):
        """Viterbi 알고리즘으로 최적 정렬 경로 찾기"""
        n_frames, n_tokens = probs.shape
        
        # DP 테이블
        dp = np.full((n_frames, len(tokens)), -np.inf)
        path = np.zeros((n_frames, len(tokens)), dtype=int)
        
        # 초기화
        dp[0, 0] = np.log(probs[0, tokens[0]])
        
        # DP 수행
        for t in range(1, n_frames):
            for j in range(len(tokens)):
                # Stay in same token
                stay_prob = dp[t-1, j] + np.log(probs[t, tokens[j]])
                
                # Move to next token
                if j > 0:
                    move_prob = dp[t-1, j-1] + np.log(probs[t, tokens[j]])
                    
                    if move_prob > stay_prob:
                        dp[t, j] = move_prob
                        path[t, j] = j - 1
                    else:
                        dp[t, j] = stay_prob
                        path[t, j] = j
                else:
                    dp[t, j] = stay_prob
                    path[t, j] = j
        
        # 백트래킹
        alignment = []
        j = len(tokens) - 1
        
        for t in range(n_frames - 1, -1, -1):
            alignment.append(tokens[j])
            j = path[t, j]
        
        alignment.reverse()
        
        return alignment
    
    def _group_tokens_to_words(self, tokens):
        """토큰을 단어로 그룹화"""
        words = []
        current_word = ""
        start_time = 0
        
        for token_info in tokens:
            token = token_info['token']
            
            if token.startswith('##'):  # WordPiece continuation
                current_word += token[2:]
            else:
                if current_word:
                    words.append({
                        'word': current_word,
                        'start': start_time,
                        'end': token_info['start']
                    })
                current_word = token
                start_time = token_info['start']
        
        # 마지막 단어 추가
        if current_word:
            words.append({
                'word': current_word,
                'start': start_time,
                'end': tokens[-1]['end'] if tokens else 0
            })
        
        return words
2.2 음절 분리기
pythonclass SyllableSplitter:
    """
    다국어 음절 분리기
    """
    
    def split_korean(self, text: str) -> List[str]:
        """
        한국어 음절 분리
        """
        syllables = []
        
        for char in text:
            if '가' <= char <= '힣':
                syllables.append(char)
            elif char.isspace():
                continue  # 공백 무시
            else:
                # 특수문자, 숫자 등은 그대로
                if syllables and not syllables[-1].isalnum():
                    syllables[-1] += char
                else:
                    syllables.append(char)
        
        return syllables
    
    def split_korean_detailed(self, text: str) -> List[Dict]:
        """
        한국어 음절 상세 분리 (초성, 중성, 종성)
        """
        CHOSUNG = ['ㄱ', 'ㄲ', 'ㄴ', 'ㄷ', 'ㄸ', 'ㄹ', 'ㅁ', 'ㅂ', 'ㅃ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅉ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']
        JUNGSUNG = ['ㅏ', 'ㅐ', 'ㅑ', 'ㅒ', 'ㅓ', 'ㅔ', 'ㅕ', 'ㅖ', 'ㅗ', 'ㅘ', 'ㅙ', 'ㅚ', 'ㅛ', 'ㅜ', 'ㅝ', 'ㅞ', 'ㅟ', 'ㅠ', 'ㅡ', 'ㅢ', 'ㅣ']
        JONGSUNG = ['', 'ㄱ', 'ㄲ', 'ㄳ', 'ㄴ', 'ㄵ', 'ㄶ', 'ㄷ', 'ㄹ', 'ㄺ', 'ㄻ', 'ㄼ', 'ㄽ', 'ㄾ', 'ㄿ', 'ㅀ', 'ㅁ', 'ㅂ', 'ㅄ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']
        
        detailed_syllables = []
        
        for char in text:
            if '가' <= char <= '힣':
                code = ord(char) - 0xAC00
                jong_idx = code % 28
                jung_idx = ((code - jong_idx) // 28) % 21
                cho_idx = ((code - jong_idx) // 28 - jung_idx) // 21
                
                detailed_syllables.append({
                    'syllable': char,
                    'chosung': CHOSUNG[cho_idx],
                    'jungsung': JUNGSUNG[jung_idx],
                    'jongsung': JONGSUNG[jong_idx] if jong_idx > 0 else None
                })
            elif not char.isspace():
                detailed_syllables.append({
                    'syllable': char,
                    'chosung': None,
                    'jungsung': None,
                    'jongsung': None
                })
        
        return detailed_syllables
    
    def split_japanese_mora(self, text: str) -> List[str]:
        """
        일본어 모라 분리
        """
        import regex
        
        # 모라 패턴
        mora_pattern = r'[ぁ-ゔー]|[ァ-ヴー]|[一-龥]|[a-zA-Z0-9]+'
        
        moras = regex.findall(mora_pattern, text)
        
        # 장음, 촉음, 발음 처리
        refined_moras = []
        i = 0
        while i < len(moras):
            current = moras[i]
            
            # 장음 처리
            if i + 1 < len(moras) and moras[i + 1] == 'ー':
                refined_moras.append(current + 'ー')
                i += 2
            # 촉음 처리
            elif current in 'っッ':
                if i + 1 < len(moras):
                    refined_moras.append(current + moras[i + 1])
                    i += 2
                else:
                    refined_moras.append(current)
                    i += 1
            else:
                refined_moras.append(current)
                i += 1
        
        return refined_moras
    
    def split_chinese_pinyin(self, text: str) -> List[Tuple[str, str]]:
        """
        중국어 병음 분리
        """
        from pypinyin import pinyin, Style
        
        result = []
        pinyin_list = pinyin(text, style=Style.TONE3)
        
        for char, py in zip(text, pinyin_list):
            if '\u4e00' <= char <= '\u9fff':  # 한자 범위
                result.append((char, py[0]))
            else:
                result.append((char, char))  # 비한자는 그대로
        
        return result

3. 한국어 특화 음절 분절
3.1 한국어 음절 분절 시스템
pythonclass KoreanSyllableSegmenter:
    """
    한국어 특화 음절 분절 시스템
    """
    
    def __init__(self):
        self.stt_engine = UniversalSTT(engine='whisper', model_size='large')
        self.aligner = SyllableAligner()
        self.splitter = SyllableSplitter()
        
    def segment_korean_audio(self, audio_file: str, 
                            use_g2p: bool = True) -> Dict:
        """
        한국어 음성 파일을 음절 단위로 분절
        
        Parameters:
        -----------
        audio_file : str
            입력 오디오 파일
        use_g2p : bool
            Grapheme-to-Phoneme 사용 여부
        
        Returns:
        --------
        Dict : 분절 결과
        """
        # 1. STT로 전사
        print("Step 1: Transcribing audio...")
        transcription = self.stt_engine.transcribe(
            audio_file, 
            language='ko',
            return_timestamps=True
        )
        
        print(f"Transcription: {transcription.text}")
        
        # 2. 음절 정렬
        print("Step 2: Aligning syllables...")
        syllables = self.aligner.align_syllables(transcription, audio_file)
        
        # 3. 음성학적 분석
        if use_g2p:
            print("Step 3: Phonetic analysis...")
            syllables = self._add_phonetic_info(syllables, audio_file)
        
        # 4. 후처리
        print("Step 4: Post-processing...")
        syllables = self._postprocess_syllables(syllables, audio_file)
        
        return {
            'transcription': transcription.text,
            'syllables': syllables,
            'statistics': self._calculate_statistics(syllables)
        }
    
    def _add_phonetic_info(self, syllables: List[Syllable], 
                          audio_file: str) -> List[Syllable]:
        """
        음성학적 정보 추가
        """
        from g2pk import G2p
        
        # G2P 초기화
        g2p = G2p()
        
        # Sound 객체 생성
        sound = parselmouth.Sound(audio_file)
        
        for syllable in syllables:
            if syllable.text:
                # 발음 변환
                pronunciation = g2p(syllable.text)
                syllable.phonemes = list(pronunciation)
                
                # 음향 특징 추출
                syllable.pitch_mean = self._extract_pitch(
                    sound, 
                    syllable.start_time, 
                    syllable.end_time
                )
                
                syllable.intensity_mean = self._extract_intensity(
                    sound, 
                    syllable.start_time, 
                    syllable.end_time
                )
                
                # 포먼트 추출
                formants = self._extract_formants(
                    sound, 
                    syllable.start_time, 
                    syllable.end_time
                )
                
                # 음절 타입 분류
                syllable_type = self._classify_syllable_type(syllable.text)
                
                # 추가 정보 저장
                syllable.metadata = {
                    'formants': formants,
                    'syllable_type': syllable_type,
                    'has_final_consonant': self._has_final_consonant(syllable.text)
                }
        
        return syllables
    
    def _extract_pitch(self, sound, start: float, end: float) -> float:
        """구간 피치 추출"""
        pitch = sound.to_pitch()
        pitch_values = []
        
        for t in np.linspace(start, end, 10):
            value = pitch.get_value_at_time(t)
            if value:
                pitch_values.append(value)
        
        return np.mean(pitch_values) if pitch_values else 0
    
    def _extract_intensity(self, sound, start: float, end: float) -> float:
        """구간 강도 추출"""
        intensity = sound.to_intensity()
        intensity_values = []
        
        for t in np.linspace(start, end, 10):
            value = call(intensity, "Get value at time", t, "cubic")
            if value:
                intensity_values.append(value)
        
        return np.mean(intensity_values) if intensity_values else 0
    
    def _extract_formants(self, sound, start: float, end: float) -> Dict:
        """구간 포먼트 추출"""
        formant = sound.to_formant_burg()
        
        formants = {'F1': [], 'F2': [], 'F3': []}
        
        for t in np.linspace(start, end, 5):
            for i, key in enumerate(['F1', 'F2', 'F3'], 1):
                value = formant.get_value_at_time(i, t)
                if value:
                    formants[key].append(value)
        
        # 평균값 계산
        for key in formants:
            formants[key] = np.mean(formants[key]) if formants[key] else 0
        
        return formants
    
    def _classify_syllable_type(self, syllable_text: str) -> str:
        """
        음절 타입 분류
        CV, CVC, V, VC 등
        """
        if not syllable_text or not ('가' <= syllable_text <= '힣'):
            return 'OTHER'
        
        # 음절 분해
        code = ord(syllable_text) - 0xAC00
        jong = code % 28
        jung = ((code - jong) // 28) % 21
        cho = ((code - jong) // 28 - jung) // 21
        
        # 초성이 ㅇ인 경우
        if cho == 11:  # ㅇ
            if jong > 0:
                return 'VC'  # Vowel + Consonant
            else:
                return 'V'   # Vowel only
        else:
            if jong > 0:
                return 'CVC' # Consonant + Vowel + Consonant
            else:
                return 'CV'  # Consonant + Vowel
    
    def _has_final_consonant(self, syllable_text: str) -> bool:
        """종성 유무 확인"""
        if not syllable_text or not ('가' <= syllable_text <= '힣'):
            return False
        
        code = ord(syllable_text) - 0xAC00
        jong = code % 28
        
        return jong > 0
    
    def _postprocess_syllables(self, syllables: List[Syllable], 
                              audio_file: str) -> List[Syllable]:
        """
        음절 후처리
        """
        # 1. 너무 짧은 음절 병합
        syllables = self._merge_short_syllables(syllables, min_duration=0.03)
        
        # 2. 묵음 구간 처리
        syllables = self._handle_silence(syllables, audio_file)
        
        # 3. 경계 조정
        syllables = self._adjust_boundaries(syllables, audio_file)
        
        return syllables
    
    def _merge_short_syllables(self, syllables: List[Syllable], 
                              min_duration: float) -> List[Syllable]:
        """짧은 음절 병합"""
        merged = []
        i = 0
        
        while i < len(syllables):
            current = syllables[i]
            duration = current.end_time - current.start_time
            
            if duration < min_duration and i + 1 < len(syllables):
                # 다음 음절과 병합
                next_syl = syllables[i + 1]
                current.end_time = next_syl.end_time
                current.text = current.text + next_syl.text
                i += 2
            else:
                merged.append(current)
                i += 1
        
        return merged
    
    def _handle_silence(self, syllables: List[Syllable], 
                       audio_file: str) -> List[Syllable]:
        """묵음 구간 처리"""
        # 음성 활동 검출
        y, sr = librosa.load(audio_file, sr=16000)
        
        # 에너지 기반 VAD
        hop_length = 512
        frame_length = 2048
        energy = librosa.feature.rms(y=y, frame_length=frame_length, 
                                    hop_length=hop_length)[0]
        
        # 묵음 임계값
        threshold = np.mean(energy) * 0.1
        
        # 음절별 체크
        filtered = []
        for syllable in syllables:
            start_frame = int(syllable.start_time * sr / hop_length)
            end_frame = int(syllable.end_time * sr / hop_length)
            
            if end_frame > start_frame:
                syllable_energy = np.mean(energy[start_frame:end_frame])
                
                if syllable_energy > threshold:
                    filtered.append(syllable)
                # 묵음이면 건너뛰기
        
        return filtered
    
    def _adjust_boundaries(self, syllables: List[Syllable], 
                          audio_file: str) -> List[Syllable]:
        """경계 미세 조정"""
        y, sr = librosa.load(audio_file, sr=16000)
        
        for i in range(len(syllables)):
            if i > 0:
                # 이전 음절과의 경계 조정
                prev_end = syllables[i-1].end_time
                curr_start = syllables[i].start_time
                
                if curr_start > prev_end:
                    # 갭이 있으면 중간점으로 조정
                    mid_point = (prev_end + curr_start) / 2
                    
                    # 영교차율 최소 지점 찾기
                    start_sample = int(prev_end * sr)
                    end_sample = int(curr_start * sr)
                    
                    if end_sample > start_sample:
                        segment = y[start_sample:end_sample]
                        zcr = librosa.feature.zero_crossing_rate(segment)[0]
                        
                        if len(zcr) > 0:
                            min_zcr_idx = np.argmin(zcr)
                            optimal_boundary = prev_end + (min_zcr_idx / len(zcr)) * (curr_start - prev_end)
                            
                            syllables[i-1].end_time = optimal_boundary
                            syllables[i].start_time = optimal_boundary
        
        return syllables
    
    def _calculate_statistics(self, syllables: List[Syllable]) -> Dict:
        """통계 계산"""
        if not syllables:
            return {}
        
        durations = [s.end_time - s.start_time for s in syllables]
        pitches = [s.pitch_mean for s in syllables if s.pitch_mean > 0]
        intensities = [s.intensity_mean for s in syllables]
        
        # 음절 타입 통계
        syllable_types = {}
        for s in syllables:
            if hasattr(s, 'metadata') and 'syllable_type' in s.metadata:
                syl_type = s.metadata['syllable_type']
                syllable_types[syl_type] = syllable_types.get(syl_type, 0) + 1
        
        return {
            'total_syllables': len(syllables),
            'total_duration': sum(durations),
            'avg_duration': np.mean(durations),
            'std_duration': np.std(durations),
            'avg_pitch': np.mean(pitches) if pitches else 0,
            'avg_intensity': np.mean(intensities),
            'syllable_types': syllable_types,
            'syllables_per_second': len(syllables) / sum(durations) if durations else 0
        }

4. 다국어 지원 시스템
4.1 다국어 음절 분절기
pythonclass MultilingualSyllableSegmenter:
    """
    다국어 음절 분절 시스템
    """
    
    def __init__(self):
        self.language_detectors = {
            'whisper': self._detect_language_whisper,
            'langdetect': self._detect_language_langdetect
        }
        
        self.segmenters = {
            'ko': KoreanSyllableSegmenter(),
            'en': EnglishSyllableSegmenter(),
            'ja': JapaneseSyllableSegmenter(),
            'zh': ChineseSyllableSegmenter(),
            'es': SpanishSyllableSegmenter(),
            'fr': FrenchSyllableSegmenter()
        }
    
    def segment(self, audio_file: str, language: str = None) -> Dict:
        """
        자동 언어 감지 및 음절 분절
        """
        # 언어 감지
        if not language:
            language = self.detect_language(audio_file)
            print(f"Detected language: {language}")
        
        # 언어별 분절기 선택
        lang_code = language.split('-')[0]  # 'ko-KR' -> 'ko'
        
        if lang_code in self.segmenters:
            segmenter = self.segmenters[lang_code]
        else:
            print(f"Language {lang_code} not fully supported, using generic segmenter")
            segmenter = GenericSyllableSegmenter()
        
        # 분절 실행
        result = segmenter.segment(audio_file)
        result['language'] = language
        
        return result
    
    def detect_language(self, audio_file: str, method: str = 'whisper') -> str:
        """언어 자동 감지"""
        if method in self.language_detectors:
            return self.language_detectors[method](audio_file)
        else:
            raise ValueError(f"Unknown language detection method: {method}")
    
    def _detect_language_whisper(self, audio_file: str) -> str:
        """Whisper로 언어 감지"""
        model = whisper.load_model("base")
        
        # 처음 30초만 로드
        audio = whisper.load_audio(audio_file)
        audio = whisper.pad_or_trim(audio)
        
        # 언어 감지
        mel = whisper.log_mel_spectrogram(audio).to(model.device)
        _, probs = model.detect_language(mel)
        
        # 가장 확률 높은 언어
        detected_language = max(probs, key=probs.get)
        
        return detected_language
    
    def _detect_language_langdetect(self, audio_file: str) -> str:
        """STT 후 텍스트 기반 언어 감지"""
        from langdetect import detect
        
        # 간단한 STT
        stt = UniversalSTT(engine='whisper', model_size='base')
        transcription = stt.transcribe(audio_file, return_timestamps=False)
        
        # 텍스트에서 언어 감지
        if transcription.text:
            lang = detect(transcription.text)
            return lang
        
        return 'unknown'

class EnglishSyllableSegmenter:
    """영어 음절 분절기"""
    
    def segment(self, audio_file: str) -> Dict:
        # 영어 특화 처리
        pass

class JapaneseSyllableSegmenter:
    """일본어 음절 분절기"""
    
    def segment(self, audio_file: str) -> Dict:
        # 일본어 모라 단위 처리
        pass

class ChineseSyllableSegmenter:
    """중국어 음절 분절기"""
    
    def segment(self, audio_file: str) -> Dict:
        # 중국어 성조 포함 처리
        pass

5. 완전 통합 파이프라인
5.1 최종 통합 시스템
pythonclass CompleteAudioSegmentationSystem:
    """
    STT + 노이즈 제거 + 음절 분절 + TextGrid 생성 통합 시스템
    """
    
    def __init__(self, config: Dict = None):
        self.config = config or self._default_config()
        
        # 컴포넌트 초기화
        self.noise_reducer = NoiseReducer()
        self.stt_engine = UniversalSTT(
            engine=self.config['stt_engine'],
            **self.config.get('stt_params', {})
        )
        self.segmenter = MultilingualSyllableSegmenter()
        self.textgrid_manager = TextGridManager()
        self.evaluator = SegmentationEvaluator()
        
    def _default_config(self) -> Dict:
        """기본 설정"""
        return {
            'stt_engine': 'whisper',
            'stt_params': {'model_size': 'large'},
            'denoise': True,
            'language': None,  # 자동 감지
            'create_textgrid': True,
            'output_format': ['textgrid', 'json', 'csv']
        }
    
    def process(self, audio_file: str, output_dir: str, 
               reference_text: str = None) -> Dict:
        """
        완전한 처리 파이프라인
        
        Parameters:
        -----------
        audio_file : str
            입력 오디오 파일
        output_dir : str
            출력 디렉토리
        reference_text : str, optional
            참조 텍스트 (제공시 강제 정렬 사용)
        
        Returns:
        --------
        Dict : 처리 결과
        """
        results = {
            'input': audio_file,
            'stages': {}
        }
        
        # 1. 노이즈 제거
        if self.config['denoise']:
            print("=" * 50)
            print("Stage 1: Noise Reduction")
            clean_audio = self._remove_noise(audio_file)
            clean_file = f"{output_dir}/clean.wav"
            self._save_audio(clean_audio['audio'], clean_file, clean_audio['sr'])
            processing_file = clean_file
            results['stages']['noise_reduction'] = {
                'output': clean_file,
                'snr_improvement': clean_audio.get('snr_improvement', 0)
            }
        else:
            processing_file = audio_file
        
        # 2. STT 전사
        print("=" * 50)
        print("Stage 2: Speech Recognition")
        
        if reference_text:
            # 참조 텍스트가 있으면 강제 정렬
            transcription = self._forced_alignment(processing_file, reference_text)
        else:
            # 자동 전사
            transcription = self.stt_engine.transcribe(
                processing_file,
                language=self.config['language'],
                return_timestamps=True
            )
        
        print(f"Transcription: {transcription.text}")
        results['stages']['transcription'] = {
            'text': transcription.text,
            'language': transcription.language,
            'confidence': transcription.confidence
        }
        
        # 3. 음절 분절
        print("=" * 50)
        print("Stage 3: Syllable Segmentation")
        
        segmentation = self.segmenter.segment(
            processing_file,
            language=transcription.language
        )
        
        # STT 결과와 음절 정렬
        if transcription.words:
            aligned_syllables = self._align_transcription_with_syllables(
                transcription,
                segmentation['syllables']
            )
            segmentation['syllables'] = aligned_syllables
        
        print(f"Segmented {len(segmentation['syllables'])} syllables")
        results['stages']['segmentation'] = segmentation
        
        # 4. TextGrid 생성
        if self.config['create_textgrid']:
            print("=" * 50)
            print("Stage 4: TextGrid Generation")
            
            textgrid_file = self._create_comprehensive_textgrid(
                segmentation['syllables'],
                transcription,
                processing_file,
                output_dir
            )
            
            results['stages']['textgrid'] = {
                'file': textgrid_file,
                'tiers': ['words', 'syllables', 'phonemes', 'pitch', 'intensity']
            }
        
        # 5. 평가 (참조 텍스트가 있는 경우)
        if reference_text:
            print("=" * 50)
            print("Stage 5: Evaluation")
            
            evaluation = self._evaluate_segmentation(
                segmentation['syllables'],
                reference_text
            )
            results['stages']['evaluation'] = evaluation
        
        # 6. 결과 저장
        self._save_results(results, output_dir)
        
        print("=" * 50)
        print("Processing complete!")
        
        return results
    
    def _remove_noise(self, audio_file: str) -> Dict:
        """노이즈 제거 with 개선도 측정"""
        y, sr = librosa.load(audio_file, sr=None)
        
        # 원본 SNR 측정
        original_snr = self._estimate_snr(y)
        
        # 노이즈 제거
        clean = self.noise_reducer.spectral_gating(y, sr)
        clean = self.noise_reducer.adaptive_filtering(clean, sr)
        
        # 개선된 SNR 측정
        clean_snr = self._estimate_snr(clean)
        
        return {
            'audio': clean,
            'sr': sr,
            'original_snr': original_snr,
            'clean_snr': clean_snr,
            'snr_improvement': clean_snr - original_snr
        }
    
    def _estimate_snr(self, audio: np.ndarray) -> float:
        """SNR 추정"""
        # 간단한 SNR 추정 (처음 10%를 노이즈로 가정)
        noise_length = len(audio) // 10
        noise = audio[:noise_length]
        signal = audio[noise_length:]
        
        noise_power = np.mean(noise ** 2)
        signal_power = np.mean(signal ** 2)
        
        if noise_power > 0:
            snr = 10 *재시도박계속편집python   def _estimate_snr(self, audio: np.ndarray) -> float:
       """SNR 추정"""
       # 간단한 SNR 추정 (처음 10%를 노이즈로 가정)
       noise_length = len(audio) // 10
       noise = audio[:noise_length]
       signal = audio[noise_length:]
       
       noise_power = np.mean(noise ** 2)
       signal_power = np.mean(signal ** 2)
       
       if noise_power > 0:
           snr = 10 * np.log10(signal_power / noise_power)
       else:
           snr = float('inf')
       
       return snr
   
   def _forced_alignment(self, audio_file: str, reference_text: str) -> TranscriptionResult:
       """참조 텍스트와 강제 정렬"""
       aligner = ForcedAlignment()
       alignment = aligner.align(audio_file, reference_text)
       
       # TranscriptionResult 형식으로 변환
       words = []
       for item in alignment:
           words.append({
               'word': item['word'],
               'start': item['start'],
               'end': item['end'],
               'confidence': 1.0  # 강제 정렬은 신뢰도 100%
           })
       
       return TranscriptionResult(
           text=reference_text,
           language=self.config.get('language', 'ko'),
           confidence=1.0,
           words=words,
           segments=[{'text': reference_text, 'start': 0, 'end': words[-1]['end'] if words else 0}]
       )
   
   def _align_transcription_with_syllables(self, transcription: TranscriptionResult,
                                          syllables: List[Syllable]) -> List[Syllable]:
       """STT 결과와 음절 정렬"""
       aligned = []
       
       # 전체 텍스트를 음절로 분리
       splitter = SyllableSplitter()
       text_syllables = splitter.split_korean(transcription.text)
       
       # 단어 타임스탬프가 있는 경우
       if transcription.words:
           word_idx = 0
           syllable_idx = 0
           
           for word_info in transcription.words:
               word = word_info['word']
               word_syllables = splitter.split_korean(word)
               
               # 단어 내 음절 균등 분배
               if word_syllables:
                   duration = (word_info['end'] - word_info['start']) / len(word_syllables)
                   
                   for i, syl_text in enumerate(word_syllables):
                       if syllable_idx < len(syllables):
                           # 기존 음절 정보 업데이트
                           syllable = syllables[syllable_idx]
                           syllable.text = syl_text
                           syllable.start_time = word_info['start'] + i * duration
                           syllable.end_time = word_info['start'] + (i + 1) * duration
                           syllable.confidence = word_info.get('confidence', 0.0)
                           aligned.append(syllable)
                           syllable_idx += 1
                       else:
                           # 새 음절 생성
                           aligned.append(Syllable(
                               start_time=word_info['start'] + i * duration,
                               end_time=word_info['start'] + (i + 1) * duration,
                               text=syl_text,
                               confidence=word_info.get('confidence', 0.0)
                           ))
       else:
           # 타임스탬프 없으면 기존 음절에 텍스트만 매핑
           for i, (syllable, text) in enumerate(zip(syllables, text_syllables)):
               syllable.text = text
               aligned.append(syllable)
       
       return aligned
   
   def _create_comprehensive_textgrid(self, syllables: List[Syllable],
                                     transcription: TranscriptionResult,
                                     audio_file: str,
                                     output_dir: str) -> str:
       """포괄적인 TextGrid 생성"""
       # Sound 분석
       sound = parselmouth.Sound(audio_file)
       duration = sound.duration
       
       # TextGrid 초기화
       self.textgrid_manager.create_textgrid(duration, {
           'words': 'IntervalTier',
           'syllables': 'IntervalTier',
           'phonemes': 'IntervalTier',
           'pitch': 'PointTier',
           'intensity': 'PointTier',
           'voicing': 'IntervalTier'
       })
       
       # 1. 단어 계층
       if transcription.words:
           self.textgrid_manager.add_words_to_tier(
               [{'start': w['start'], 'end': w['end'], 'text': w['word']} 
                for w in transcription.words]
           )
       
       # 2. 음절 계층
       self.textgrid_manager.add_syllables_to_tier(syllables)
       
       # 3. 음소 계층
       phonemes = self._extract_phonemes(syllables)
       if phonemes:
           tier = self.textgrid_manager.tiers['phonemes']
           for phoneme in phonemes:
               interval = tgt.Interval(
                   start_time=phoneme['start'],
                   end_time=phoneme['end'],
                   text=phoneme['text']
               )
               tier.add_interval(interval)
       
       # 4. 피치 계층
       pitch = sound.to_pitch()
       pitch_points = []
       for syllable in syllables:
           mid_time = (syllable.start_time + syllable.end_time) / 2
           pitch_value = pitch.get_value_at_time(mid_time)
           if pitch_value:
               pitch_points.append((mid_time, pitch_value))
       
       self.textgrid_manager.add_pitch_points(pitch_points)
       
       # 5. 강도 계층
       intensity = sound.to_intensity()
       intensity_points = []
       for syllable in syllables:
           mid_time = (syllable.start_time + syllable.end_time) / 2
           intensity_value = call(intensity, "Get value at time", mid_time, "cubic")
           if intensity_value:
               intensity_points.append((mid_time, intensity_value))
       
       tier = self.textgrid_manager.tiers['intensity']
       for time, value in intensity_points:
           point = tgt.Point(time=time, text=f"{value:.1f}")
           tier.add_point(point)
       
       # 6. 유성/무성 구분
       voicing = self._detect_voicing(sound, syllables)
       tier = self.textgrid_manager.tiers['voicing']
       for v in voicing:
           interval = tgt.Interval(
               start_time=v['start'],
               end_time=v['end'],
               text=v['type']
           )
           tier.add_interval(interval)
       
       # 저장
       output_file = f"{output_dir}/comprehensive.TextGrid"
       self.textgrid_manager.save_textgrid(output_file)
       
       # JSON 백업
       json_file = f"{output_dir}/comprehensive.json"
       self.textgrid_manager.export_to_json(json_file)
       
       return output_file
   
   def _extract_phonemes(self, syllables: List[Syllable]) -> List[Dict]:
       """음절에서 음소 추출"""
       phonemes = []
       
       for syllable in syllables:
           if hasattr(syllable, 'phonemes') and syllable.phonemes:
               # 음소별 균등 분배
               duration = (syllable.end_time - syllable.start_time) / len(syllable.phonemes)
               
               for i, phoneme in enumerate(syllable.phonemes):
                   phonemes.append({
                       'start': syllable.start_time + i * duration,
                       'end': syllable.start_time + (i + 1) * duration,
                       'text': phoneme
                   })
       
       return phonemes
   
   def _detect_voicing(self, sound, syllables: List[Syllable]) -> List[Dict]:
       """유성/무성 구간 검출"""
       voicing = []
       
       # HNR 기반 검출
       harmonicity = call(sound, "To Harmonicity (cc)", 0.01, 75, 0.1, 1.0)
       
       for syllable in syllables:
           mid_time = (syllable.start_time + syllable.end_time) / 2
           hnr = call(harmonicity, "Get value at time", mid_time, "cubic")
           
           if hnr and hnr > 0:
               voicing_type = 'voiced'
           else:
               voicing_type = 'unvoiced'
           
           voicing.append({
               'start': syllable.start_time,
               'end': syllable.end_time,
               'type': voicing_type
           })
       
       return voicing
   
   def _evaluate_segmentation(self, syllables: List[Syllable],
                             reference_text: str) -> Dict:
       """분절 결과 평가"""
       # 참조 텍스트를 음절로 분리
       splitter = SyllableSplitter()
       reference_syllables = splitter.split_korean(reference_text)
       
       # 텍스트 정확도
       predicted_text = ''.join([s.text for s in syllables if s.text])
       reference_clean = reference_text.replace(' ', '')
       
       # 편집 거리
       from difflib import SequenceMatcher
       similarity = SequenceMatcher(None, predicted_text, reference_clean).ratio()
       
       # 음절 수 비교
       syllable_count_accuracy = 1 - abs(len(syllables) - len(reference_syllables)) / len(reference_syllables)
       
       return {
           'text_similarity': similarity,
           'syllable_count_accuracy': syllable_count_accuracy,
           'predicted_syllables': len(syllables),
           'reference_syllables': len(reference_syllables),
           'predicted_text': predicted_text,
           'reference_text': reference_clean
       }
   
   def _save_results(self, results: Dict, output_dir: str):
       """결과 저장"""
       # JSON 저장
       json_file = f"{output_dir}/results.json"
       with open(json_file, 'w', encoding='utf-8') as f:
           # numpy 배열 등을 JSON serializable하게 변환
           def convert(obj):
               if isinstance(obj, np.ndarray):
                   return obj.tolist()
               elif isinstance(obj, Syllable):
                   return {
                       'start_time': obj.start_time,
                       'end_time': obj.end_time,
                       'text': obj.text,
                       'confidence': obj.confidence,
                       'pitch_mean': obj.pitch_mean,
                       'intensity_mean': obj.intensity_mean
                   }
               elif hasattr(obj, '__dict__'):
                   return obj.__dict__
               return obj
           
           json.dump(results, f, indent=2, ensure_ascii=False, default=convert)
       
       # CSV 저장 (음절 정보)
       if 'segmentation' in results['stages'] and 'syllables' in results['stages']['segmentation']:
           csv_file = f"{output_dir}/syllables.csv"
           
           import csv
           with open(csv_file, 'w', newline='', encoding='utf-8') as f:
               writer = csv.writer(f)
               writer.writerow(['index', 'text', 'start_time', 'end_time', 
                              'duration', 'pitch', 'intensity', 'confidence'])
               
               for i, syl in enumerate(results['stages']['segmentation']['syllables']):
                   writer.writerow([
                       i,
                       syl.text,
                       syl.start_time,
                       syl.end_time,
                       syl.end_time - syl.start_time,
                       syl.pitch_mean,
                       syl.intensity_mean,
                       syl.confidence
                   ])
       
       # 요약 리포트 생성
       report_file = f"{output_dir}/report.txt"
       with open(report_file, 'w', encoding='utf-8') as f:
           f.write("=" * 60 + "\n")
           f.write("Audio Segmentation Report\n")
           f.write("=" * 60 + "\n\n")
           
           f.write(f"Input File: {results['input']}\n")
           f.write(f"Processing Time: {time.strftime('%Y-%m-%d %H:%M:%S')}\n\n")
           
           if 'transcription' in results['stages']:
               trans = results['stages']['transcription']
               f.write("Transcription:\n")
               f.write("-" * 40 + "\n")
               f.write(f"Text: {trans['text']}\n")
               f.write(f"Language: {trans['language']}\n")
               f.write(f"Confidence: {trans['confidence']:.2%}\n\n")
           
           if 'segmentation' in results['stages']:
               seg = results['stages']['segmentation']
               if 'statistics' in seg:
                   stats = seg['statistics']
                   f.write("Segmentation Statistics:\n")
                   f.write("-" * 40 + "\n")
                   f.write(f"Total Syllables: {stats.get('total_syllables', 0)}\n")
                   f.write(f"Total Duration: {stats.get('total_duration', 0):.2f}s\n")
                   f.write(f"Avg Syllable Duration: {stats.get('avg_duration', 0):.3f}s\n")
                   f.write(f"Syllables per Second: {stats.get('syllables_per_second', 0):.2f}\n")
                   
                   if 'syllable_types' in stats:
                       f.write("\nSyllable Types:\n")
                       for syl_type, count in stats['syllable_types'].items():
                           f.write(f"  {syl_type}: {count}\n")
           
           if 'evaluation' in results['stages']:
               eval_data = results['stages']['evaluation']
               f.write("\nEvaluation Results:\n")
               f.write("-" * 40 + "\n")
               f.write(f"Text Similarity: {eval_data['text_similarity']:.2%}\n")
               f.write(f"Syllable Count Accuracy: {eval_data['syllable_count_accuracy']:.2%}\n")

   def _save_audio(self, audio: np.ndarray, output_file: str, sr: int):
       """오디오 파일 저장"""
       import soundfile as sf
       sf.write(output_file, audio, sr)
5.2 GUI 인터페이스
pythonimport tkinter as tk
from tkinter import ttk, filedialog, messagebox
import threading

class AudioSegmentationGUI:
    """
    그래픽 사용자 인터페이스
    """
    
    def __init__(self):
        self.root = tk.Tk()
        self.root.title("Audio Syllable Segmentation System")
        self.root.geometry("800x600")
        
        self.system = CompleteAudioSegmentationSystem()
        self.setup_ui()
    
    def setup_ui(self):
        """UI 설정"""
        # 메인 프레임
        main_frame = ttk.Frame(self.root, padding="10")
        main_frame.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))
        
        # 파일 선택
        ttk.Label(main_frame, text="Audio File:").grid(row=0, column=0, sticky=tk.W)
        self.file_var = tk.StringVar()
        ttk.Entry(main_frame, textvariable=self.file_var, width=50).grid(row=0, column=1)
        ttk.Button(main_frame, text="Browse", command=self.browse_file).grid(row=0, column=2)
        
        # 언어 선택
        ttk.Label(main_frame, text="Language:").grid(row=1, column=0, sticky=tk.W)
        self.lang_var = tk.StringVar(value="auto")
        lang_combo = ttk.Combobox(main_frame, textvariable=self.lang_var)
        lang_combo['values'] = ('auto', 'ko', 'en', 'ja', 'zh', 'es', 'fr')
        lang_combo.grid(row=1, column=1, sticky=tk.W)
        
        # STT 엔진 선택
        ttk.Label(main_frame, text="STT Engine:").grid(row=2, column=0, sticky=tk.W)
        self.engine_var = tk.StringVar(value="whisper")
        engine_combo = ttk.Combobox(main_frame, textvariable=self.engine_var)
        engine_combo['values'] = ('whisper', 'google', 'azure', 'naver_clova')
        engine_combo.grid(row=2, column=1, sticky=tk.W)
        
        # 옵션
        self.denoise_var = tk.BooleanVar(value=True)
        ttk.Checkbutton(main_frame, text="Remove Noise", 
                       variable=self.denoise_var).grid(row=3, column=0, sticky=tk.W)
        
        self.textgrid_var = tk.BooleanVar(value=True)
        ttk.Checkbutton(main_frame, text="Create TextGrid", 
                       variable=self.textgrid_var).grid(row=3, column=1, sticky=tk.W)
        
        # 참조 텍스트
        ttk.Label(main_frame, text="Reference Text (optional):").grid(row=4, column=0, sticky=tk.W)
        self.ref_text = tk.Text(main_frame, height=5, width=60)
        self.ref_text.grid(row=5, column=0, columnspan=3)
        
        # 진행 바
        self.progress = ttk.Progressbar(main_frame, mode='indeterminate')
        self.progress.grid(row=6, column=0, columnspan=3, sticky=(tk.W, tk.E), pady=10)
        
        # 처리 버튼
        ttk.Button(main_frame, text="Process", 
                  command=self.process_audio).grid(row=7, column=1)
        
        # 결과 표시
        ttk.Label(main_frame, text="Results:").grid(row=8, column=0, sticky=tk.W)
        self.result_text = tk.Text(main_frame, height=10, width=70)
        self.result_text.grid(row=9, column=0, columnspan=3)
        
        # 스크롤바
        scrollbar = ttk.Scrollbar(main_frame, orient="vertical", 
                                 command=self.result_text.yview)
        scrollbar.grid(row=9, column=3, sticky=(tk.N, tk.S))
        self.result_text.configure(yscrollcommand=scrollbar.set)
    
    def browse_file(self):
        """파일 선택 대화상자"""
        filename = filedialog.askopenfilename(
            title="Select Audio File",
            filetypes=[("Audio Files", "*.wav *.mp3 *.m4a *.flac"),
                      ("All Files", "*.*")]
        )
        if filename:
            self.file_var.set(filename)
    
    def process_audio(self):
        """오디오 처리 시작"""
        audio_file = self.file_var.get()
        if not audio_file:
            messagebox.showerror("Error", "Please select an audio file")
            return
        
        # 설정 업데이트
        self.system.config['language'] = None if self.lang_var.get() == 'auto' else self.lang_var.get()
        self.system.config['stt_engine'] = self.engine_var.get()
        self.system.config['denoise'] = self.denoise_var.get()
        self.system.config['create_textgrid'] = self.textgrid_var.get()
        
        # 참조 텍스트
        ref_text = self.ref_text.get("1.0", tk.END).strip()
        
        # 출력 디렉토리
        output_dir = filedialog.askdirectory(title="Select Output Directory")
        if not output_dir:
            return
        
        # 진행 바 시작
        self.progress.start()
        self.result_text.delete("1.0", tk.END)
        self.result_text.insert("1.0", "Processing...\n")
        
        # 백그라운드 스레드에서 처리
        thread = threading.Thread(
            target=self._process_in_background,
            args=(audio_file, output_dir, ref_text or None)
        )
        thread.start()
    
    def _process_in_background(self, audio_file, output_dir, ref_text):
        """백그라운드 처리"""
        try:
            results = self.system.process(audio_file, output_dir, ref_text)
            
            # 결과 표시
            self.root.after(0, self._display_results, results)
        except Exception as e:
            self.root.after(0, self._display_error, str(e))
        finally:
            self.root.after(0, self.progress.stop)
    
    def _display_results(self, results):
        """결과 표시"""
        self.result_text.delete("1.0", tk.END)
        
        text = "Processing Complete!\n\n"
        
        if 'transcription' in results['stages']:
            trans = results['stages']['transcription']
            text += f"Transcription:\n{trans['text']}\n\n"
            text += f"Language: {trans['language']}\n"
            text += f"Confidence: {trans['confidence']:.2%}\n\n"
        
        if 'segmentation' in results['stages']:
            seg = results['stages']['segmentation']
            if 'statistics' in seg:
                stats = seg['statistics']
                text += f"Syllables: {stats.get('total_syllables', 0)}\n"
                text += f"Duration: {stats.get('total_duration', 0):.2f}s\n"
                text += f"Avg Duration: {stats.get('avg_duration', 0):.3f}s\n\n"
        
        if 'textgrid' in results['stages']:
            text += f"TextGrid saved: {results['stages']['textgrid']['file']}\n"
        
        self.result_text.insert("1.0", text)
    
    def _display_error(self, error_msg):
        """에러 표시"""
        self.result_text.delete("1.0", tk.END)
        self.result_text.insert("1.0", f"Error: {error_msg}")
        messagebox.showerror("Processing Error", error_msg)
    
    def run(self):
        """GUI 실행"""
        self.root.mainloop()
5.3 CLI 인터페이스
python@click.command()
@click.argument('audio_file', type=click.Path(exists=True))
@click.option('--output-dir', '-o', default='./output', help='Output directory')
@click.option('--language', '-l', help='Language code (auto-detect if not specified)')
@click.option('--engine', '-e', 
              type=click.Choice(['whisper', 'google', 'azure', 'naver']),
              default='whisper', help='STT engine')
@click.option('--reference', '-r', help='Reference text file for forced alignment')
@click.option('--denoise/--no-denoise', default=True, help='Apply noise reduction')
@click.option('--textgrid/--no-textgrid', default=True, help='Create TextGrid output')
@click.option('--verbose', '-v', is_flag=True, help='Verbose output')
def segment_audio(audio_file, output_dir, language, engine, 
                 reference, denoise, textgrid, verbose):
    """
    Segment audio file into syllables with STT
    """
    # 시스템 초기화
    config = {
        'stt_engine': engine,
        'language': language,
        'denoise': denoise,
        'create_textgrid': textgrid
    }
    
    system = CompleteAudioSegmentationSystem(config)
    
    # 참조 텍스트 로드
    ref_text = None
    if reference:
        with open(reference, 'r', encoding='utf-8') as f:
            ref_text = f.read().strip()
    
    # 처리 실행
    try:
        results = system.process(audio_file, output_dir, ref_text)
        
        # 결과 출력
        if verbose:
            click.echo(json.dumps(results, indent=2, ensure_ascii=False))
        else:
            # 요약 출력
            if 'transcription' in results['stages']:
                click.echo(f"Transcription: {results['stages']['transcription']['text']}")
            
            if 'segmentation' in results['stages']:
                stats = results['stages']['segmentation'].get('statistics', {})
                click.echo(f"Syllables: {stats.get('total_syllables', 0)}")
                click.echo(f"Duration: {stats.get('total_duration', 0):.2f}s")
        
        click.echo(f"\nResults saved to: {output_dir}")
        
    except Exception as e:
        click.echo(f"Error: {e}", err=True)
        raise click.Abort()

if __name__ == "__main__":
    # GUI 또는 CLI 선택
    import sys
    
    if len(sys.argv) == 1:
        # 인자 없으면 GUI 실행
        gui = AudioSegmentationGUI()
        gui.run()
    else:
        # 인자 있으면 CLI 실행
        segment_audio()
결론
이제 완전한 음절 분절 시스템이 구축되었습니다:
핵심 기능

STT 통합: 다양한 엔진 지원 (Whisper, Google, Azure, Naver CLOVA)
자동 언어 감지: 다국어 자동 처리
텍스트-음성 정렬: 정확한 음절 타이밍
노이즈 제거: 깨끗한 음성 처리
TextGrid 생성: 다층 구조의 annotation

장점

정확도: STT 결과를 활용한 정확한 음절 분절
다국어: 한국어, 영어, 일본어, 중국어 등 지원
유연성: 참조 텍스트 유무에 관계없이 동작
확장성: 새로운 언어나 STT 엔진 추가 용이
사용성: GUI와 CLI 모두 제공

활용 분야

음성 인식 시스템 개발
언어 학습 애플리케이션
자막 생성 시스템
음성학 연구
TTS 데이터 준비

이 시스템으로 텍스트 정보 없이도 정확한 음절 분절이 가능하며, STT를 통해 얻은 텍스트로 더욱 정밀한 분절이 가능합니다.